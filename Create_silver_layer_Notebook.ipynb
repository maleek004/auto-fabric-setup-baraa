{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02669d2-6ef7-4354-98d2-7f24abdd5cb2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:14.7519357Z",
       "execution_start_time": "2025-10-23T17:29:14.2692471Z",
       "normalized_state": "finished",
       "parent_msg_id": "413f1986-8e14-40b0-927d-571873568ead",
       "queued_time": "2025-10-23T17:29:14.2682646Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workspaceId = ''\n",
    "lakehouseId = ''\n",
    "timezone = \"Africa/Lagos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import IPython ## not importing display because it conflicts with Fabric notebook's display() function\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from datetime import datetime , date\n",
    "import pytz\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "#Quick variables\n",
    "my_tz = pytz.timezone(timezone)\n",
    "my_tz_NOW = datetime.now(my_tz)\n",
    "today = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3eb1e-de15-4f4a-ac28-8b8843374c76",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:23.6002463Z",
       "execution_start_time": "2025-10-23T17:29:14.7531244Z",
       "normalized_state": "finished",
       "parent_msg_id": "12e18e97-5a1e-48e5-9524-c787ab2aad43",
       "queued_time": "2025-10-23T17:29:14.5866073Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating empty silver layer table definitions\n",
    "# Using this to mimic the TRUNCATE -> INSERT data load method\n",
    "\n",
    "\n",
    "\n",
    "abfs_path = f'abfss://{workspaceId}@onelake.dfs.fabric.microsoft.com/{lakehouseId}'\n",
    "# crm_cust_info\n",
    "crm_cust_info_schema = [\n",
    "    (\"cst_id\", pl.Int16),\n",
    "    (\"cst_key\", pl.Utf8),\n",
    "    (\"cst_firstname\", pl.Utf8),\n",
    "    (\"cst_lastname\", pl.Utf8),\n",
    "    (\"cst_marital_status\", pl.Utf8),\n",
    "    (\"cst_gndr\", pl.Utf8),\n",
    "    (\"cst_create_date\", pl.Date),\n",
    "    (\"dwh_create_date\", pl.Datetime(\"us\", time_zone=\"UTC\"))\n",
    "]\n",
    "crm_cust_info = pl.DataFrame(schema=crm_cust_info_schema)\n",
    "crm_cust_info.write_delta(f\"{abfs_path}/Tables/crm_cust_info\", mode=\"overwrite\")\n",
    "\n",
    "# crm_prd_info\n",
    "crm_prd_info_schema = [\n",
    "    (\"prd_id\", pl.Int16),\n",
    "    (\"cat_id\", pl.Utf8),\n",
    "    (\"prd_key\", pl.Utf8),\n",
    "    (\"prd_nm\", pl.Utf8),\n",
    "    (\"prd_cost\", pl.Int16),\n",
    "    (\"prd_line\", pl.Utf8),\n",
    "    (\"prd_start_dt\", pl.Date),\n",
    "    (\"prd_end_dt\", pl.Date),\n",
    "    (\"dwh_create_date\", pl.Datetime(\"us\", time_zone=\"UTC\"))\n",
    "]\n",
    "crm_prd_info = pl.DataFrame(schema=crm_prd_info_schema)\n",
    "crm_prd_info.write_delta(f\"{abfs_path}/Tables/crm_prd_info\", mode=\"overwrite\")\n",
    "\n",
    "# crm_sales_details\n",
    "crm_sales_details_schema = [\n",
    "    (\"sls_ord_num\", pl.Utf8),\n",
    "    (\"sls_prd_key\", pl.Utf8),\n",
    "    (\"sls_cust_id\", pl.Int32),\n",
    "    (\"sls_order_dt\", pl.Date),\n",
    "    (\"sls_ship_dt\", pl.Date),\n",
    "    (\"sls_due_dt\", pl.Date),\n",
    "    (\"sls_sales\", pl.Int16),\n",
    "    (\"sls_quantity\", pl.Int16),\n",
    "    (\"sls_price\", pl.Int16),\n",
    "    (\"dwh_create_date\", pl.Datetime(\"us\", time_zone=\"UTC\"))\n",
    "]\n",
    "crm_sales_details = pl.DataFrame(schema=crm_sales_details_schema)\n",
    "crm_sales_details.write_delta(f\"{abfs_path}/Tables/crm_sales_details\", mode=\"overwrite\")\n",
    "\n",
    "# erp_loc_a101\n",
    "erp_loc_a101_schema = [\n",
    "    (\"cid\", pl.Utf8),\n",
    "    (\"cntry\", pl.Utf8),\n",
    "    (\"dwh_create_date\", pl.Datetime(\"us\", time_zone=\"UTC\"))\n",
    "]\n",
    "erp_loc_a101 = pl.DataFrame(schema=erp_loc_a101_schema)\n",
    "erp_loc_a101.write_delta(f\"{abfs_path}/Tables/erp_loc_a101\", mode=\"overwrite\",delta_write_options={\"schema_mode\":\"overwrite\"})\n",
    "\n",
    "# erp_cust_az12\n",
    "erp_cust_az12_schema = [\n",
    "    (\"cid\", pl.Utf8),\n",
    "    (\"bdate\", pl.Date),\n",
    "    (\"gen\", pl.Utf8),\n",
    "    (\"dwh_create_date\", pl.Datetime(\"us\", time_zone=\"UTC\"))\n",
    "]\n",
    "erp_cust_az12 = pl.DataFrame(schema=erp_cust_az12_schema)\n",
    "erp_cust_az12.write_delta(f\"{abfs_path}/Tables/erp_cust_az12\", mode=\"overwrite\")\n",
    "\n",
    "# erp_px_cat_g1v2\n",
    "erp_px_cat_g1v2_schema = [\n",
    "    (\"id\", pl.Utf8),\n",
    "    (\"cat\", pl.Utf8),\n",
    "    (\"subcat\", pl.Utf8),\n",
    "    (\"maintenance\", pl.Utf8),\n",
    "    (\"dwh_create_date\", pl.Datetime(\"us\", time_zone=\"UTC\"))\n",
    "]\n",
    "erp_px_cat_g1v2 = pl.DataFrame(schema=erp_px_cat_g1v2_schema)\n",
    "erp_px_cat_g1v2.write_delta(f\"{abfs_path}/Tables/erp_px_cat_g1v2\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed3e1a-8e67-4e4b-889f-7113688cced1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:24.9879842Z",
       "execution_start_time": "2025-10-23T17:29:23.6013955Z",
       "normalized_state": "finished",
       "parent_msg_id": "2c0518f0-dddd-47d2-8e69-be6240b97d9d",
       "queued_time": "2025-10-23T17:29:15.2040849Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading csv files into dtaframe \n",
    "\n",
    "crm_cust_info = pd.read_csv(f\"{abfs_path}/Files/datasets/source_crm/cust_info.csv\")\n",
    "crm_prd_info = pd.read_csv(f\"{abfs_path}/Files/datasets/source_crm/prd_info.csv\")\n",
    "crm_sales_details = pd.read_csv(f\"{abfs_path}/Files/datasets/source_crm/sales_details.csv\")\n",
    "erp_cust_az12 = pd.read_csv(f\"{abfs_path}/Files/datasets/source_erp/CUST_AZ12.csv\")\n",
    "erp_loc_a101 = pd.read_csv(f\"{abfs_path}/Files/datasets/source_erp/LOC_A101.csv\")\n",
    "erp_px_cat_g1v2 = pd.read_csv(f\"{abfs_path}/Files/datasets/source_erp/PX_CAT_G1V2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed515b32-40d4-4eed-8137-c79ba1c7a7dc",
   "metadata": {},
   "source": [
    "## Transformation steps for the crm_cust_info dataframe\n",
    "### Issues to fix:\n",
    "- Some `cst_id` values appear more than once,  We want to keep only the latest record per customer based on the `cst_create_date`column. the column also contains some null values\n",
    "- Remove whitespaces from the `cst_firstname` and `cst_lastname` columns \n",
    "- Replace **â€˜Fâ€™** with **â€˜Femaleâ€™** , **â€˜Mâ€™** with **â€˜Maleâ€™**  and **Null** with **â€˜n/aâ€™** in the `cst_gndr` column\n",
    "- Replace **â€˜Sâ€™** with **'Singleâ€™** , **â€˜Mâ€™** with **â€˜Marriedâ€™**  and **Null** with **â€˜n/aâ€™** in the `cst_marital_status` column\n",
    "- Add the `dwh_create_date` column to the dataframe \n",
    "- Make sure the data types matches the sce=hema definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25900c1-9a42-4a61-a422-5ad6b90b8f1b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:25.3548747Z",
       "execution_start_time": "2025-10-23T17:29:24.9892434Z",
       "normalized_state": "finished",
       "parent_msg_id": "2ddfec1e-d0d1-4644-b785-ffdbf1bb272a",
       "queued_time": "2025-10-23T17:29:16.1089963Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Removing rows with null cst_id and drop duplicate customer IDs (by keeping latest by cst_create_date)......\n",
      "Before: 4 null cst_id(s)\n",
      "Before: 9 duplicate cst_id(s)\n",
      "After: 0 null cst_id(s)\n",
      "After: 0 duplicate cst_id(s)\n",
      "\n",
      "Checking, cleaning, and confirming whitespaces in cst_firstname and cst_lastname columns.........\n",
      "17 rows in cst_firstname contain leading/trailing whitespace\n",
      "22 rows in cst_lastname contain leading/trailing whitespace\n",
      "Now, 0 rows in cst_firstname contains whitespace (should be 0)\n",
      "Now, 0 rows in cst_lastname contains whitespace (should be 0)\n",
      "\n",
      "Performing standardization on cst_gndr column................\n",
      "Unique values before standardization: [nan 'M' 'F']\n",
      "Unique values after standardization: ['n/a' 'Male' 'Female']\n",
      "\n",
      "Performing standardization on cst_marital_status column.............\n",
      "Unique values before standardization: ['M' 'S']\n",
      "Unique values after standardization: ['Married' 'Single']\n",
      "\n",
      "Adding 'dwh_create_date' column with current WAT datetime..........\n",
      "'dwh_create_date' column added with current datetime: 2025-10-23 18:29:23.680819+01:00\n",
      "\n",
      "Performing Type conversion to match table schema.................\n",
      "Before: cst_id type = float64, cst_create_date type = object\n",
      "After: cst_id type = int16, cst_create_date type = object\n",
      "\n",
      "Dataset shape before transformations is: (18494, 7)\n",
      "Dataset shape after transformations is: (18484, 8)\n"
     ]
    }
   ],
   "source": [
    "def transform_crm_cust_info(df):\n",
    "    start_shape = df.shape\n",
    "# Remove rows with null cst_id and drop duplicate customer IDs (by keeping latest by cst_create_date)    \n",
    "    print(\"# Removing rows with null cst_id and drop duplicate customer IDs (by keeping latest by cst_create_date)......\")\n",
    "    print(f\"Before: {df['cst_id'].isnull().sum()} null cst_id(s)\")\n",
    "    print(f\"Before: {df.duplicated(subset='cst_id').sum()} duplicate cst_id(s)\")\n",
    "\n",
    "    df = (df.dropna(subset=['cst_id'])\n",
    "           .sort_values(\"cst_create_date\", ascending=False)\n",
    "           .drop_duplicates(subset=\"cst_id\", keep=\"first\") )\n",
    "\n",
    "    print(f\"After: {df['cst_id'].isnull().sum()} null cst_id(s)\")\n",
    "    print(f\"After: {df.duplicated(subset='cst_id').sum()} duplicate cst_id(s)\")\n",
    "\n",
    "# Check, clean, and confirm whitespaces in cst_firstname and cst_lastname\n",
    "    print(\"\\nChecking, cleaning, and confirming whitespaces in cst_firstname and cst_lastname columns.........\")\n",
    "    first_name_ws = (df[\"cst_firstname\"] != df[\"cst_firstname\"].str.strip()).sum()\n",
    "    last_name_ws = (df[\"cst_lastname\"] != df[\"cst_lastname\"].str.strip()).sum()\n",
    "    print(f\"{first_name_ws} rows in cst_firstname contain leading/trailing whitespace\")\n",
    "    print(f\"{last_name_ws} rows in cst_lastname contain leading/trailing whitespace\")\n",
    "\n",
    "    df[\"cst_firstname\"] = df[\"cst_firstname\"].str.strip()\n",
    "    df[\"cst_lastname\"] = df[\"cst_lastname\"].str.strip()\n",
    "\n",
    "    print(f\"Now, {(df['cst_firstname'] != df['cst_firstname'].str.strip()).sum()} rows in cst_firstname contains whitespace (should be 0)\")\n",
    "    print(f\"Now, {(df['cst_lastname'] != df['cst_lastname'].str.strip()).sum()} rows in cst_lastname contains whitespace (should be 0)\")\n",
    "\n",
    "# Check, clean, and confirm standardization in cst_gndr\n",
    "    print(\"\\nPerforming standardization on cst_gndr column................\")\n",
    "    print(\"Unique values before standardization:\", df[\"cst_gndr\"].unique())\n",
    "    df[\"cst_gndr\"] = df[\"cst_gndr\"].map({\"F\": \"Female\", \"M\": \"Male\"}).fillna(\"n/a\")\n",
    "    print(\"Unique values after standardization:\", df[\"cst_gndr\"].unique())\n",
    "\n",
    "# Check, clean, and confirm standardization in cst_marital_status\n",
    "    print(\"\\nPerforming standardization on cst_marital_status column.............\")\n",
    "    print(\"Unique values before standardization:\", df[\"cst_marital_status\"].unique())\n",
    "    df[\"cst_marital_status\"] = df[\"cst_marital_status\"].map({\"S\": \"Single\", \"M\": \"Married\"}).fillna(\"n/a\")\n",
    "    print(\"Unique values after standardization:\", df[\"cst_marital_status\"].unique())\n",
    "\n",
    "# Creating a  dwh_create_date column as current WAT date and time\n",
    "    print(\"\\nAdding 'dwh_create_date' column with current WAT datetime..........\")\n",
    "    df[\"dwh_create_date\"] = pd.to_datetime(my_tz_NOW)\n",
    "    print(\"'dwh_create_date' column added with current datetime:\", df[\"dwh_create_date\"].iloc[0])\n",
    "\n",
    "# Type conversion to match table schema\n",
    "    print(\"\\nPerforming Type conversion to match table schema.................\")\n",
    "    print(f\"Before: cst_id type = {df['cst_id'].dtype}, cst_create_date type = {df['cst_create_date'].dtype}\")\n",
    "    df[\"cst_id\"] = pd.to_numeric(df[\"cst_id\"], downcast=\"integer\")\n",
    "    df[\"cst_create_date\"] = pd.to_datetime(df[\"cst_create_date\"]).dt.date\n",
    "    #df[\"dwh_create_date\"] = pd.to_datetime(df[\"dwh_create_date\"]).dt.date\n",
    "    print(f\"After: cst_id type = {df['cst_id'].dtype}, cst_create_date type = {df['cst_create_date'].dtype}\")\n",
    "\n",
    "    end_shape = df.shape\n",
    "    print(f\"\\nDataset shape before transformations is: {start_shape}\\nDataset shape after transformations is: {end_shape}\")\n",
    "\n",
    "    return df\n",
    "crm_cust_info = transform_crm_cust_info(crm_cust_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89add7f-0c5a-492d-9cab-1211f828a968",
   "metadata": {},
   "source": [
    "## Transformation steps for the crm_prd_info dataframe\n",
    "- Create a cat_id column by extracting the first 5 characters in the prd_key column, and replacing the â€˜-â€™ in them with a â€˜_â€™\n",
    "- Created a new prd_key column by extracting every character from the 7th position in the original prd_key column\n",
    "- Replaced null with 0 in the prd_cost column\n",
    "- Replaced â€˜Mâ€™ with â€˜Mountainâ€™, â€˜Râ€™ with â€˜Roadsâ€™ , â€˜Sâ€™ with â€˜other salesâ€™ ,â€™Tâ€™ with â€˜Touringâ€™ and Null with â€˜n/aâ€™  from the prd_line column\n",
    "- Removed the original prd_end_date column and recreated another one by selecting the record in the next row of the prd_start_date column for each prd_key partition, and subtract the value by one i.e one day before the next start_date\n",
    "- Converted the data type of both the prd_start_date and prd_end_date columns from datetime to date\n",
    "Add the dwh_create_date column to the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20c232-9fe7-46de-bb95-b64580213d5a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:25.7044161Z",
       "execution_start_time": "2025-10-23T17:29:25.3560796Z",
       "normalized_state": "finished",
       "parent_msg_id": "5e103541-7152-44e7-ab5a-f7e135fb5059",
       "queued_time": "2025-10-23T17:29:16.3178489Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 'cat_id' column from first 5 characters of 'prd_key'................\n",
      " This is what it looks like now:\n",
      "            prd_key cat_id\n",
      "0  CO-RF-FR-R92B-58  CO_RF\n",
      "1  CO-RF-FR-R92R-58  CO_RF\n",
      "\n",
      "Transforming 'prd_key' by slicing from the 7th character .........\n",
      "This is what it look like now, (compare it to the output of the previous transformation step above):\n",
      "      prd_key\n",
      "0  FR-R92B-58\n",
      "1  FR-R92R-58\n",
      "\n",
      "Replacing nulls in 'prd_cost' column with 0............\n",
      "Number of nulls before replacement: 2\n",
      "Number of nulls after replacement: 0\n",
      "\n",
      "Standardizing values in 'prd_line' column...........\n",
      "Unique values before standardization: ['R ' 'S ' 'M ' nan 'T ']\n",
      "Unique values after standardization: ['Roads' 'other sales' 'Mountain' 'n/a' 'Touring']\n",
      "\n",
      "recreating the prd_end_dt column............\n",
      "This is what it currently looks like before transfromation \n",
      "     prd_key prd_start_dt  prd_end_dt\n",
      "2  HL-U509-R   2011-07-01  2007-12-28\n",
      "3  HL-U509-R   2012-07-01  2008-12-27\n",
      "5    HL-U509   2011-07-01  2007-12-28\n",
      "6    HL-U509   2012-07-01  2008-12-27\n",
      "8  SO-B909-M   2011-07-01  2007-12-28\n",
      "\n",
      "This is what it looks like after transformation:\n",
      "      prd_key prd_start_dt prd_end_dt\n",
      "2   HL-U509-R   2011-07-01 2012-06-30\n",
      "3   HL-U509-R   2012-07-01 2013-06-30\n",
      "5     HL-U509   2011-07-01 2012-06-30\n",
      "6     HL-U509   2012-07-01 2013-06-30\n",
      "10  HL-U509-B   2011-07-01 2012-06-30\n",
      "\n",
      "Adding 'dwh_create_date' column to the dataframe...............\n",
      "dwh_create_date column added with current datetime: 2025-10-23 18:29:23.680819+01:00\n",
      "\n",
      "Performing Type conversion to match table schema.................\n",
      "Before: prd_id type = int64, prd_cost type = float64, prd_start_dt type = datetime64[ns], prd_end_dt type = datetime64[ns], dwh_create_date type = datetime64[ns, Africa/Lagos]\n",
      "After: prd_id type = int16, prd_cost type = int16, prd_start_dt type = object, prd_end_dt type = object, dwh_create_date type = datetime64[ns, Africa/Lagos]\n",
      "\n",
      "Dataset shape before transformations is: (397, 7)\n",
      "Dataset shape after transformations is: (397, 9)\n"
     ]
    }
   ],
   "source": [
    "def transform_crm_prd_info(crm_prd_info):\n",
    "    start_shape = crm_prd_info.shape\n",
    "    # extractung cat_id from prd_key column \n",
    "    print(\"Creating 'cat_id' column from first 5 characters of 'prd_key'................\\n This is what it looks like now:\")\n",
    "    crm_prd_info[\"cat_id\"] = crm_prd_info[\"prd_key\"].str[:5].str.replace(\"-\", \"_\", regex=False)\n",
    "    print(crm_prd_info[[\"prd_key\", \"cat_id\"]].head(2))\n",
    "\n",
    "    # Slicing prd_key column\n",
    "    print(\"\\nTransforming 'prd_key' by slicing from the 7th character .........\\nThis is what it look like now, (compare it to the output of the previous transformation step above):\")\n",
    "    crm_prd_info[\"prd_key\"] = crm_prd_info[\"prd_key\"].str[6:]\n",
    "    print(crm_prd_info[[\"prd_key\"]].head(2))\n",
    "\n",
    "    # Replace nulls in the prd_cost column\n",
    "    print(f\"\\nReplacing nulls in 'prd_cost' column with 0............\\nNumber of nulls before replacement: {crm_prd_info['prd_cost'].isnull().sum()}\")\n",
    "    crm_prd_info[\"prd_cost\"] = crm_prd_info[\"prd_cost\"].fillna(0)\n",
    "    print(f\"Number of nulls after replacement: {crm_prd_info['prd_cost'].isnull().sum()}\")\n",
    "\n",
    "    # Standardizing the prd_line column\n",
    "    print(\"\\nStandardizing values in 'prd_line' column...........\\nUnique values before standardization:\", crm_prd_info[\"prd_line\"].unique())\n",
    "    crm_prd_info[\"prd_line\"] = (\n",
    "        crm_prd_info[\"prd_line\"]\n",
    "        .astype(str)\n",
    "        .str.upper()\n",
    "        .map({\n",
    "            \"M \": \"Mountain\",\n",
    "            \"R \": \"Roads\",\n",
    "            \"S \": \"other sales\",\n",
    "            \"T \": \"Touring\"\n",
    "        })\n",
    "        .fillna(\"n/a\") \n",
    "        )\n",
    "    print(\"Unique values after standardization:\", crm_prd_info[\"prd_line\"].unique())\n",
    "\n",
    "    # Recreating the prd_end_dt column to fix errors\n",
    "    print(\"\\nrecreating the prd_end_dt column............\\nThis is what it currently looks like before transfromation \")\n",
    "    crm_prd_info[\"prd_start_dt\"] = pd.to_datetime(crm_prd_info[\"prd_start_dt\"])\n",
    "    quick_check = crm_prd_info[crm_prd_info[\"prd_end_dt\"].notnull()][[\"prd_key\",\"prd_start_dt\", \"prd_end_dt\"]].head(5)\n",
    "    print(quick_check)\n",
    "\n",
    "    crm_prd_info = crm_prd_info.sort_values(by=[\"prd_key\", \"prd_start_dt\"])\n",
    "    crm_prd_info[\"next_start_dt\"] = crm_prd_info.groupby(\"prd_key\")[\"prd_start_dt\"].shift(-1)\n",
    "    crm_prd_info[\"prd_end_dt\"] = crm_prd_info[\"next_start_dt\"] - pd.Timedelta(days=1)\n",
    "    crm_prd_info = crm_prd_info.drop(columns=[\"next_start_dt\"])\n",
    "    crm_prd_info = crm_prd_info.sort_values(\"prd_id\")\n",
    "\n",
    "    print(\"\\nThis is what it looks like after transformation:\")\n",
    "    quick_check = crm_prd_info[crm_prd_info[\"prd_end_dt\"].notnull()][[\"prd_key\",\"prd_start_dt\", \"prd_end_dt\"]].head(5)\n",
    "    print(quick_check)\n",
    "\n",
    "    # Adding the dwh_create_date column\n",
    "    print(\"\\nAdding 'dwh_create_date' column to the dataframe...............\")\n",
    "    crm_prd_info[\"dwh_create_date\"] = pd.to_datetime(my_tz_NOW)\n",
    "    print(\"dwh_create_date column added with current datetime:\", crm_prd_info[\"dwh_create_date\"].iloc[0])\n",
    "\n",
    "    # Type conversion to match defined schema\n",
    "    print(\"\\nPerforming Type conversion to match table schema.................\")\n",
    "    print(f\"Before: prd_id type = {crm_prd_info['prd_id'].dtype}, prd_cost type = {crm_prd_info['prd_cost'].dtype}, prd_start_dt type = {crm_prd_info['prd_start_dt'].dtype}, prd_end_dt type = {crm_prd_info['prd_end_dt'].dtype}, dwh_create_date type = {crm_prd_info['dwh_create_date'].dtype}\")\n",
    "\n",
    "    crm_prd_info[\"prd_id\"] = pd.to_numeric(crm_prd_info[\"prd_id\"], downcast=\"integer\")\n",
    "    crm_prd_info[\"prd_cost\"] = pd.to_numeric(crm_prd_info[\"prd_cost\"], downcast=\"integer\")\n",
    "    crm_prd_info[\"prd_start_dt\"] = pd.to_datetime(crm_prd_info[\"prd_start_dt\"]).dt.date\n",
    "    crm_prd_info[\"prd_end_dt\"] = pd.to_datetime(crm_prd_info[\"prd_end_dt\"]).dt.date\n",
    "    #crm_prd_info[\"dwh_create_date\"] = pd.to_datetime(crm_prd_info[\"dwh_create_date\"]).dt.date\n",
    "\n",
    "    print(f\"After: prd_id type = {crm_prd_info['prd_id'].dtype}, prd_cost type = {crm_prd_info['prd_cost'].dtype}, prd_start_dt type = {crm_prd_info['prd_start_dt'].dtype}, prd_end_dt type = {crm_prd_info['prd_end_dt'].dtype}, dwh_create_date type = {crm_prd_info['dwh_create_date'].dtype}\")\n",
    "\n",
    "    end_shape = crm_prd_info.shape\n",
    "\n",
    "    print(f\"\\nDataset shape before transformations is: {start_shape}\\nDataset shape after transformations is: {end_shape}\")\n",
    "    \n",
    "\n",
    "    return crm_prd_info\n",
    "\n",
    "\n",
    "crm_prd_info = transform_crm_prd_info(crm_prd_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4581d16-2be7-49bb-ac54-2b713f80897e",
   "metadata": {},
   "source": [
    "## Transformations for the crm_sales_details dataframe\n",
    "- Check the integrity of the sls_prd_key (FK) column by checking if there are any sls_prd_key that are not available in the primary key column of the related crm_prd_info table (i.e the prd_key column)\n",
    "- Check the integrity of the sls_cust_id (Foreign Key) column by checking if there are any sls_cust_id that are not available in the primary key column of the related crm_cust_info table (i.e the cst_id column) \n",
    "- Replace every record in the sls_order_date column where the length of the value is not equal to 8 or the value is 0 with Null. and converted the remaining values from integer to date type\n",
    "- Create a new sls_sales column by multiplying the sls_quantity column with the absolute value of sls_price column \n",
    "- Create a new sls_price column by dividing the new sls_sales column by the sls_quantity column (and handling cases where the value in quantity column might be 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ade32a-b148-4837-8530-d435e8e54cd1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:27.0751499Z",
       "execution_start_time": "2025-10-23T17:29:25.7057027Z",
       "normalized_state": "finished",
       "parent_msg_id": "fa22773c-3f09-4fd0-ac61-f40bbeabe0b3",
       "queued_time": "2025-10-23T17:29:16.574326Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking integrity of the sls_prd_key foreign key column...........\n",
      "All sls_prd_key values have a matching prd_key in crm_prd_info. ðŸ˜ŽðŸ˜Ž\n",
      "\n",
      "Checking Foreign Key Integrity for sls_cust_id column....................\n",
      "ðŸ˜Ž All sls_cust_id values have matching cst_id values in crm_cust_info.\n",
      "\n",
      "Cleaning the sls_order_dt column.............\n",
      "Number of error values in sls_order_dt before transformation: 19\n",
      "This is what they look like before cleaning: \n",
      " 35319    0\n",
      "35320    0\n",
      "35321    0\n",
      "35322    0\n",
      "35323    0\n",
      "Name: sls_order_dt, dtype: object\n",
      "Number of errors values in sls_order_dt after transformation (must be 0): 0\n",
      "\n",
      "Ensuring the sls_ship_dt column too is clean.............\n",
      "ðŸ˜Žno errors in the sls_ship_dt column sire !!\n",
      "\n",
      "Checking for date inconsistencies........\n",
      "ðŸ˜Ž All good! No date inconsistencies found.\n",
      "\n",
      "Cleaning the sls_sales, and sls_price columns..........\n",
      "Before transformation, number of rows where sls_sales is null or < 0 or != sls_quantity * sls_price: 35 \n",
      "this is what they look like:\n",
      "       sls_sales  sls_quantity  sls_price\n",
      "20142      769.0             1     -769.0\n",
      "46329      -54.0             1       54.0\n",
      "28860        NaN             1        8.0\n",
      "45548        NaN             1       24.0\n",
      "46697      -35.0             1       35.0\n",
      "After cleaning, number of rows where sls_sales is null or < 0 or != sls_quantity * sls_price:  0\n",
      "\n",
      "Adding 'dwh_create_date' column to the dataframe...............\n",
      "dwh_create_date column added with current datetime: 2025-10-23 18:29:23.680819+01:00\n",
      "\n",
      "Performing Type conversion to match table schema.................\n",
      "data types before transformation\n",
      " {'sls_ord_num': dtype('O'), 'sls_prd_key': dtype('O'), 'sls_cust_id': dtype('int64'), 'sls_order_dt': dtype('<M8[ns]'), 'sls_ship_dt': dtype('<M8[ns]'), 'sls_due_dt': dtype('<M8[ns]'), 'sls_sales': dtype('float64'), 'sls_quantity': dtype('int64'), 'sls_price': dtype('float64'), 'dwh_create_date': datetime64[ns, Africa/Lagos]}\n",
      "\n",
      "data types after transformation:\n",
      " {'sls_ord_num': dtype('O'), 'sls_prd_key': dtype('O'), 'sls_cust_id': dtype('int32'), 'sls_order_dt': dtype('O'), 'sls_ship_dt': dtype('O'), 'sls_due_dt': dtype('O'), 'sls_sales': dtype('int16'), 'sls_quantity': dtype('int16'), 'sls_price': dtype('int16'), 'dwh_create_date': datetime64[ns, Africa/Lagos]}\n",
      "\n",
      "Dataset shape before transformations is: (60398, 9)\n",
      "Dataset shape after transformations is: (60398, 10)\n"
     ]
    }
   ],
   "source": [
    "def transform_crm_sales_details(crm_sales_details):\n",
    "    start_shape = crm_sales_details.shape\n",
    "    \n",
    "    #STEP 1 check FK integrity - sls_prd_key\n",
    "    print(\"Checking integrity of the sls_prd_key foreign key column...........\")\n",
    "    unique_sls_prd_keys = crm_sales_details[\"sls_prd_key\"].unique()\n",
    "    valid_prd_keys = crm_prd_info[\"prd_key\"].unique()\n",
    "    orphaned_prd_keys = set(unique_sls_prd_keys) - set(valid_prd_keys)\n",
    "    #validate\n",
    "    if orphaned_prd_keys:\n",
    "        invalid_sales_records = crm_sales_details[crm_sales_details[\"sls_prd_key\"].isin(orphaned_prd_keys)]\n",
    "        print(\"Sample of records with invalid sls_prd_key:\")\n",
    "        print(invalid_sales_records.head(5))\n",
    "    else:\n",
    "        print(\"All sls_prd_key values have a matching prd_key in crm_prd_info. ðŸ˜ŽðŸ˜Ž\")\n",
    "    \n",
    "\n",
    "    #STEP 2: Check FK integrity - sls_cust_id\n",
    "    print('\\nChecking Foreign Key Integrity for sls_cust_id column....................')\n",
    "    unique_sls_cust_ids = crm_sales_details[\"sls_cust_id\"].unique()\n",
    "    valid_cust_ids = crm_cust_info[\"cst_id\"].unique()\n",
    "    orphaned_cust_ids = set(unique_sls_cust_ids) - set(valid_cust_ids)\n",
    "    #validate\n",
    "    if orphaned_cust_ids:\n",
    "        invalid_sales = crm_sales_details[crm_sales_details[\"sls_cust_id\"].isin(orphaned_cust_ids)]\n",
    "        print(\"Sample records with invalid sls_cust_id:\")\n",
    "        print(invalid_sales.head())\n",
    "    else:\n",
    "        print('ðŸ˜Ž All sls_cust_id values have matching cst_id values in crm_cust_info.')\n",
    "    \n",
    "\n",
    "    #STEP 3: Clean the sls_order_dt column\n",
    "    print(\"\\nCleaning the sls_order_dt column.............\")\n",
    "    crm_sales_details[\"sls_order_dt\"] = crm_sales_details[\"sls_order_dt\"].astype(str)\n",
    "    error_mask = (crm_sales_details[\"sls_order_dt\"].str.len() != 8) | (crm_sales_details[\"sls_order_dt\"] == \"0\")\n",
    "    print(\"Number of error values in sls_order_dt before transformation:\", error_mask.sum())\n",
    "    print(\"This is what they look like before cleaning: \\n\",crm_sales_details[error_mask].sls_order_dt.head())\n",
    "\n",
    "    # ensure the date columns are datetime\n",
    "    crm_sales_details[\"sls_due_dt\"] = pd.to_datetime(crm_sales_details[\"sls_due_dt\"], errors=\"coerce\", format=\"%Y%m%d\")\n",
    "    crm_sales_details[\"sls_order_dt\"] = pd.to_datetime(crm_sales_details[\"sls_order_dt\"], errors=\"coerce\", format=\"%Y%m%d\")\n",
    "    \n",
    "    # fix order date\n",
    "    crm_sales_details.loc[error_mask, \"sls_order_dt\"] = (crm_sales_details.loc[error_mask, \"sls_due_dt\"] - pd.Timedelta(days=12))\n",
    "\n",
    "    # finally convert to datetime\n",
    "    crm_sales_details[\"sls_order_dt\"] = pd.to_datetime(crm_sales_details[\"sls_order_dt\"], errors=\"coerce\", format=\"%Y%m%d\")\n",
    "    #validate\n",
    "    print(\"Number of errors values in sls_order_dt after transformation (must be 0):\", crm_sales_details[\"sls_order_dt\"].isna().sum())\n",
    "\n",
    "\n",
    "    #STEP 4: Clean sls_ship_dt column\n",
    "    print(\"\\nEnsuring the sls_ship_dt column too is clean.............\")\n",
    "    crm_sales_details[\"sls_ship_dt\"] = crm_sales_details[\"sls_ship_dt\"].astype(str)\n",
    "    error_mask = (crm_sales_details[\"sls_ship_dt\"].str.len() != 8) | (crm_sales_details[\"sls_ship_dt\"] == \"0\")\n",
    "    if error_mask.any():\n",
    "        print(\"Number of error values in sls_ship_dt before transformation:\", error_mask.sum())\n",
    "        print(\"This is what they look like before cleaning: \\n\",crm_sales_details[error_mask].head())\n",
    "\n",
    "        crm_sales_details[\"sls_ship_dt\"] = crm_sales_details[\"sls_ship_dt\"].apply(\n",
    "            lambda x: x if len(x) == 8 and x != \"0\" else None  )\n",
    "\n",
    "        crm_sales_details[\"sls_ship_dt\"] = pd.to_datetime(\n",
    "            crm_sales_details[\"sls_ship_dt\"], \n",
    "            format=\"%Y%m%d\" )\n",
    "\n",
    "    #validate\n",
    "        print(\"Number of Null values in sls_ship_dt after transformation:\", crm_sales_details[\"sls_ship_dt\"].isna().sum())\n",
    "        print(\"Sample after cleaning:\")\n",
    "        print(crm_sales_details[\"sls_ship_dt\"].head())\n",
    "    else:\n",
    "        print(\"ðŸ˜Žno errors in the sls_ship_dt column sire !!\")\n",
    "        crm_sales_details[\"sls_ship_dt\"] = pd.to_datetime(crm_sales_details[\"sls_ship_dt\"], format=\"%Y%m%d\")\n",
    "\n",
    "\n",
    "    #STEP 4: Check for date inconsistencies\n",
    "    print(\"\\nChecking for date inconsistencies........\")\n",
    "    crm_sales_details['sls_due_dt'] = pd.to_datetime(crm_sales_details['sls_due_dt'], format=\"%Y%m%d\")\n",
    "    order_after_ship = crm_sales_details[\"sls_order_dt\"] > crm_sales_details[\"sls_ship_dt\"]\n",
    "    order_after_due = (crm_sales_details[\"sls_due_dt\"] != pd.to_datetime('1970-01-01')) & (crm_sales_details[\"sls_order_dt\"] > crm_sales_details[\"sls_due_dt\"])\n",
    "    date_issues_mask = order_after_ship | order_after_due\n",
    "\n",
    "    #Validate \n",
    "    if date_issues_mask.any():\n",
    "        print(\"âš ï¸ Found date inconsistencies where order date is later than ship or due date:\")\n",
    "        print(f\"- Number of rows where order date > ship date: {order_after_ship.sum()}\")\n",
    "        print(f\"- Number of rows where order date > due date: {order_after_due.sum()}\")\n",
    "        print(\"Here are some problematic rows:\\n\")\n",
    "        print(crm_sales_details[date_issues_mask][[\"sls_order_dt\", \"sls_ship_dt\", \"sls_due_dt\"]].head())\n",
    "    else:\n",
    "        print(\"ðŸ˜Ž All good! No date inconsistencies found.\")\n",
    "\n",
    "\n",
    "\n",
    "    #STEP 5: Clean sls_sales and sls_price columns\n",
    "    print(\"\\nCleaning the sls_sales, and sls_price columns..........\")\n",
    "    invalid_sales_mask = (\n",
    "        crm_sales_details[\"sls_sales\"].isnull() |\n",
    "        (crm_sales_details[\"sls_sales\"] < 0) |  \n",
    "        (crm_sales_details[\"sls_sales\"] != crm_sales_details[\"sls_quantity\"] * crm_sales_details[\"sls_price\"])  \n",
    "        )\n",
    "    invalid_sales_count = invalid_sales_mask.sum()\n",
    "    print(\"Before transformation, number of rows where sls_sales is null or < 0 or != sls_quantity * sls_price:\",invalid_sales_count,'\\nthis is what they look like:')\n",
    "    print(crm_sales_details[invalid_sales_mask][[ 'sls_sales' , 'sls_quantity' , 'sls_price']].sample(n=5))\n",
    "    crm_sales_details['sls_price'] = crm_sales_details.apply(lambda row: round(abs(row[\"sls_sales\"]) / row[\"sls_quantity\"],2) if pd.notnull(row[\"sls_sales\"]) else row[\"sls_price\"],axis=1)\n",
    "    crm_sales_details['sls_sales'] = crm_sales_details.sls_price * crm_sales_details.sls_quantity\n",
    "\n",
    "    # Validate\n",
    "    invalid_sales_mask = (\n",
    "        crm_sales_details[\"sls_sales\"].isnull() |  \n",
    "        (crm_sales_details[\"sls_sales\"] < 0) |  \n",
    "        (crm_sales_details[\"sls_sales\"] != crm_sales_details[\"sls_quantity\"] * crm_sales_details[\"sls_price\"])\n",
    "    )\n",
    "    invalid_sales_count = invalid_sales_mask.sum()\n",
    "    print(\"After cleaning, number of rows where sls_sales is null or < 0 or != sls_quantity * sls_price: \",invalid_sales_count)\n",
    "\n",
    "\n",
    "    #STEP 6:Add the dwh_create_date column\n",
    "    print(\"\\nAdding 'dwh_create_date' column to the dataframe...............\")\n",
    "    crm_sales_details[\"dwh_create_date\"] = pd.to_datetime(my_tz_NOW)\n",
    "    print(\"dwh_create_date column added with current datetime:\", crm_sales_details[\"dwh_create_date\"].iloc[0])\n",
    "\n",
    "\n",
    "    #STEP 7:Type conversion to match defined schema\n",
    "    print(\"\\nPerforming Type conversion to match table schema.................\")\n",
    "    print(\"data types before transformation\\n\",crm_sales_details.dtypes.to_dict())\n",
    "    #clean\n",
    "    crm_sales_details['sls_cust_id'] = crm_sales_details['sls_cust_id'].astype('int32')\n",
    "    crm_sales_details['sls_order_dt'] = pd.to_datetime(crm_sales_details['sls_order_dt']).dt.date\n",
    "    crm_sales_details['sls_ship_dt'] = pd.to_datetime(crm_sales_details['sls_ship_dt']).dt.date\n",
    "    crm_sales_details['sls_due_dt'] = pd.to_datetime(crm_sales_details['sls_due_dt']).dt.date\n",
    "    #crm_sales_details['dwh_create_date'] = pd.to_datetime(crm_sales_details['dwh_create_date']).dt.date\n",
    "    crm_sales_details['sls_sales'] = crm_sales_details['sls_sales'].astype('int16')\n",
    "    crm_sales_details['sls_quantity'] = crm_sales_details['sls_quantity'].astype('int16')\n",
    "    crm_sales_details['sls_price'] = crm_sales_details['sls_price'].astype('int16')\n",
    "\n",
    "    #validate\n",
    "    print(\"\\ndata types after transformation:\\n\",crm_sales_details.dtypes.to_dict())\n",
    "\n",
    "    end_shape = crm_sales_details.shape\n",
    "\n",
    "    print(f\"\\nDataset shape before transformations is: {start_shape}\\nDataset shape after transformations is: {end_shape}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    return crm_sales_details\n",
    "\n",
    "crm_sales_details = transform_crm_sales_details(crm_sales_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4cf0e-7bc8-438d-a702-a1e55f68dcf1",
   "metadata": {},
   "source": [
    "## Transformation steps for the erp_cust_az12 dataframe\n",
    "- remove the string 'NAS' from the cid column\n",
    "- Replace birtdates in the future with null \n",
    "- standardize gender column\n",
    "- Add the dwh_create_date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15de13-429e-4cb9-aec0-563d68367188",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:27.4548805Z",
       "execution_start_time": "2025-10-23T17:29:27.0764591Z",
       "normalized_state": "finished",
       "parent_msg_id": "beefa9a1-6bd5-40fb-85f4-ca7ed3172f68",
       "queued_time": "2025-10-23T17:29:16.7963631Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing the string 'NAS' from the cid column............\n",
      "Unique Length of values in the cid column before transformation:  [13 10]\n",
      "Unique Length of values in the cid column after transformation:  [10]\n",
      "\n",
      "replacing future burthdays with null .................,,,,,,,,,,,,\n",
      "Number of nulls before transformation:  5\n",
      "number of birth dates greater than today before transformation:  11\n",
      "Number of nulls after transformation:  16\n",
      "number of birth dates greater than today after transformation:  0\n",
      "\n",
      "Standardizing values in the 'gen' column....................\n",
      "Unique values before transformation:  ['Male' 'Female' '  ' ' ' 'M ' 'F ' 'F' 'M' nan]\n",
      "Unique values in 'GEN' after cleaning: ['Male' 'Female' 'n/a']\n",
      "\n",
      "Adding 'dwh_create_date' column to the dataframe...............\n",
      "dwh_create_date column added with current datetime: 2025-10-23 18:29:23.680819+01:00\n",
      "\n",
      "Dataset shape before transformations is: (18484, 3)\n",
      "Dataset shape after transformations is: (18484, 4)\n"
     ]
    }
   ],
   "source": [
    "def transform_erp_cust_az12(erp_cust_az12):\n",
    "    start_shape = erp_cust_az12.shape\n",
    "    # remove the string 'NAS' from the cid column\n",
    "    print(\"removing the string 'NAS' from the cid column............\")\n",
    "    print('Unique Length of values in the cid column before transformation: ',erp_cust_az12['CID'].astype('str').str.len().unique())\n",
    "    erp_cust_az12['CID'] = erp_cust_az12['CID'].astype('str').str.replace('NAS','',regex=False).str.strip()\n",
    "    #check\n",
    "    print('Unique Length of values in the cid column after transformation: ',erp_cust_az12['CID'].astype('str').str.len().unique())\n",
    "\n",
    "    #Replace birtdates in the future with null \n",
    "    print('\\nreplacing future burthdays with null .................,,,,,,,,,,,,')\n",
    "    erp_cust_az12['BDATE'] = pd.to_datetime(erp_cust_az12['BDATE'], errors='coerce').dt.date\n",
    "    print('Number of nulls before transformation: ',erp_cust_az12['BDATE'].isnull().sum())\n",
    "    noFutureDates = (erp_cust_az12['BDATE'] > today).sum()\n",
    "    print(\"number of birth dates greater than today before transformation: \",noFutureDates)\n",
    "    erp_cust_az12.loc[(erp_cust_az12['BDATE'] > today), 'BDATE'] = pd.NaT\n",
    "    noFutureDates = (erp_cust_az12['BDATE'] > today).sum()\n",
    "    #check\n",
    "    print('Number of nulls after transformation: ',erp_cust_az12['BDATE'].isnull().sum())\n",
    "    print(\"number of birth dates greater than today after transformation: \", noFutureDates)\n",
    "\n",
    "    #standardize gender column\n",
    "    print(\"\\nStandardizing values in the 'gen' column....................\")\n",
    "    print(\"Unique values before transformation: \", erp_cust_az12.GEN.unique())\n",
    "    erp_cust_az12['GEN'] = erp_cust_az12['GEN'].astype(str).str.strip().str.lower()\n",
    "    gender_map = {'m': 'Male','male': 'Male','f': 'Female','female': 'Female','': 'n/a','nan': 'n/a'}\n",
    "    erp_cust_az12['GEN'] = erp_cust_az12['GEN'].map(gender_map)\n",
    "    # Check\n",
    "    print(\"Unique values in 'GEN' after cleaning:\", erp_cust_az12['GEN'].unique())\n",
    "\n",
    "    #Add the dwh_create_date column\n",
    "    print(\"\\nAdding 'dwh_create_date' column to the dataframe...............\")\n",
    "    erp_cust_az12[\"dwh_create_date\"] = pd.to_datetime(my_tz_NOW)\n",
    "    print(\"dwh_create_date column added with current datetime:\", erp_cust_az12[\"dwh_create_date\"].iloc[0])\n",
    "\n",
    "    erp_cust_az12.columns = ['cid', 'bdate', 'gen', 'dwh_create_date']    \n",
    "    \n",
    "    \n",
    "    end_shape = erp_cust_az12.shape\n",
    "    print(f\"\\nDataset shape before transformations is: {start_shape}\\nDataset shape after transformations is: {end_shape}\")\n",
    "\n",
    "    \n",
    "    return erp_cust_az12\n",
    "\n",
    "\n",
    "erp_cust_az12 = transform_erp_cust_az12(erp_cust_az12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05338edd-cd50-4e49-93d4-e6d0957ad762",
   "metadata": {},
   "source": [
    "## Transformation for erp_loc_a101\n",
    "- Remove â€˜-â€™ from the cid column \n",
    "- Standardize the cntry column \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a35ccf-8aae-4001-8bd5-144f04053a64",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:27.8091291Z",
       "execution_start_time": "2025-10-23T17:29:27.4561104Z",
       "normalized_state": "finished",
       "parent_msg_id": "716c6f0e-053c-459e-a330-107469624ab4",
       "queued_time": "2025-10-23T17:29:17.0764265Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing '-' characters from the 'cid' column.........\n",
      "Sample cleaned values from 'cid':\n",
      "7046     AW00018046\n",
      "5207     AW00016207\n",
      "6506     AW00017506\n",
      "16371    AW00027371\n",
      "4943     AW00015943\n",
      "Name: cid, dtype: object\n",
      "\n",
      "Standardizing the 'cntry' column............\n",
      "Unique values before transformation:  ['Australia' 'US' 'Canada' 'DE' 'United Kingdom' 'France' 'USA' 'Germany'\n",
      " 'nan' '' 'United States']\n",
      "Unique values in 'cntry' after standardization:  ['Australia' 'United States' 'Canada' 'Germany' 'United Kingdom' 'France']\n",
      "\n",
      "Adding 'dwh_create_date' column to the dataframe...............\n",
      "dwh_create_date column added with current datetime: 2025-10-23 18:29:23.680819+01:00\n",
      "\n",
      "Dataset shape before transformations is: (18484, 2)\n",
      "Dataset shape after transformations is: (18484, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def transform_erp_loc_a101(erp_loc_a101):\n",
    "    start_shape = erp_loc_a101.shape\n",
    "    erp_loc_a101.columns = ['cid','cntry']\n",
    "    # Replace dashes with empty string\n",
    "    print(\"Removing '-' characters from the 'cid' column.........\")\n",
    "    erp_loc_a101['cid'] = erp_loc_a101['cid'].astype(str).str.replace('-', '', regex=False)\n",
    "\n",
    "    # validate\n",
    "    print(\"Sample cleaned values from 'cid':\")\n",
    "    print(erp_loc_a101['cid'].sample(5))\n",
    "\n",
    "    #Standardize the cntry column and add a random choice for 'unknown' countries \n",
    "    print(\"\\nStandardizing the 'cntry' column............\")\n",
    "    erp_loc_a101['cntry'] = erp_loc_a101['cntry'].astype(str).str.strip()\n",
    "    print(\"Unique values before transformation: \", erp_loc_a101.cntry.unique())\n",
    "    countries = ['Australia', 'United States', 'Canada', 'Germany', 'United Kingdom', 'France']\n",
    "    country_map = {\n",
    "        'DE': 'Germany',\n",
    "        'US': 'United States',\n",
    "        'USA': 'United States',\n",
    "    }\n",
    "\n",
    "    def standardize_countries(val):\n",
    "        if pd.isnull(val) or val in ['', 'nan']:\n",
    "            return random.choice(countries)\n",
    "        else:\n",
    "            return country_map.get(val, val)\n",
    "    \n",
    "    erp_loc_a101['cntry'] = erp_loc_a101['cntry'].apply(standardize_countries)\n",
    "\n",
    "    #validate\n",
    "    print(\"Unique values in 'cntry' after standardization: \", erp_loc_a101['cntry'].unique())\n",
    "\n",
    "    #creating the dwh_create_date column\n",
    "    print(\"\\nAdding 'dwh_create_date' column to the dataframe...............\")\n",
    "    erp_loc_a101[\"dwh_create_date\"] = pd.to_datetime(my_tz_NOW)\n",
    "    print(\"dwh_create_date column added with current datetime:\", erp_loc_a101[\"dwh_create_date\"].iloc[0])\n",
    "    \n",
    "    end_shape = erp_loc_a101.shape\n",
    "    print(f\"\\nDataset shape before transformations is: {start_shape}\\nDataset shape after transformations is: {end_shape}\")\n",
    "\n",
    "    \n",
    "\n",
    "    return erp_loc_a101\n",
    "\n",
    "erp_loc_a101 = transform_erp_loc_a101(erp_loc_a101 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff95e3-f94a-4a92-9420-6dac54d9b648",
   "metadata": {},
   "source": [
    "## Transformations for the erp_px_cat_g1v2 dataset\n",
    "* Add the dwh_create_date to the dataset\n",
    "* perform data type conversion to match the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372c513-f770-4e60-b188-2a3706ca8943",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:28.2084066Z",
       "execution_start_time": "2025-10-23T17:29:27.8103309Z",
       "normalized_state": "finished",
       "parent_msg_id": "53a090fc-abd0-416d-b8ce-4c870a8e7a34",
       "queued_time": "2025-10-23T17:29:17.3126266Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the missing category informaton 'CO_PE'....................\n",
      "\n",
      "adding 'dwh_create_date' column to the dataframe.................\n",
      "dwh_create_date column added with current datetime: 2025-10-23 18:29:23.680819+01:00\n",
      "\n",
      "Dataset shape before transformations is: (37, 4)\n",
      "Dataset shape after transformations is: (38, 5)\n"
     ]
    }
   ],
   "source": [
    "def transform_erp_px_cat_g1v2(erp_px_cat_g1v2):\n",
    "    start_shape = erp_px_cat_g1v2.shape\n",
    "\n",
    "    #Adding the missing category informaton 'CO_PE'\n",
    "    print(\"Adding the missing category informaton 'CO_PE'....................\")\n",
    "    erp_px_cat_g1v2.loc[len(erp_px_cat_g1v2)] = ['CO_PE','Components','Pedals','Yes']\n",
    "    #renaming columns to match defined schema\n",
    "    erp_px_cat_g1v2.columns = ['id','cat','subcat','maintenance']\n",
    "    \n",
    "    # Adding the dwh_create_date column\n",
    "    print(\"\\nadding 'dwh_create_date' column to the dataframe.................\")\n",
    "    erp_px_cat_g1v2['dwh_create_date'] = pd.to_datetime(my_tz_NOW)\n",
    "    print(\"dwh_create_date column added with current datetime:\", erp_px_cat_g1v2[\"dwh_create_date\"].iloc[0])\n",
    "\n",
    "    end_shape = erp_px_cat_g1v2.shape\n",
    "    print(f\"\\nDataset shape before transformations is: {start_shape}\\nDataset shape after transformations is: {end_shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    return erp_px_cat_g1v2\n",
    "\n",
    "erp_px_cat_g1v2 = transform_erp_px_cat_g1v2(erp_px_cat_g1v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e150be4b-ce2d-4dec-b626-86f656bebbe5",
   "metadata": {},
   "source": [
    "### Creating Date dimenson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92408af-0bb1-43ff-b225-e823c67a25ad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:28.5430933Z",
       "execution_start_time": "2025-10-23T17:29:28.2096374Z",
       "normalized_state": "finished",
       "parent_msg_id": "bf383d9a-b041-44ef-ba9f-985673ecdfff",
       "queued_time": "2025-10-23T17:29:17.6256263Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-12-29 2014-01-28\n"
     ]
    }
   ],
   "source": [
    "start_date = str(min(crm_sales_details['sls_order_dt']))\n",
    "end_date= str(max(crm_sales_details['sls_order_dt']))\n",
    "print(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8727d-8a18-415f-873a-883e1158047e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:28.9579659Z",
       "execution_start_time": "2025-10-23T17:29:28.5442411Z",
       "normalized_state": "finished",
       "parent_msg_id": "116f6a0e-fc7e-4ce1-a470-2376f85f103e",
       "queued_time": "2025-10-23T17:29:17.8174336Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Day</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Weekday2</th>\n",
       "      <th>Month</th>\n",
       "      <th>MonthName</th>\n",
       "      <th>Monthshort</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Year</th>\n",
       "      <th>Year_Month</th>\n",
       "      <th>Month_Year</th>\n",
       "      <th>EOM</th>\n",
       "      <th>EndofQuarter</th>\n",
       "      <th>EOQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-12-29</td>\n",
       "      <td>29</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010_12</td>\n",
       "      <td>December 2010</td>\n",
       "      <td>False</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-12-30</td>\n",
       "      <td>30</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010_12</td>\n",
       "      <td>December 2010</td>\n",
       "      <td>False</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>31</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010_12</td>\n",
       "      <td>December 2010</td>\n",
       "      <td>True</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011_01</td>\n",
       "      <td>January 2011</td>\n",
       "      <td>False</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011_01</td>\n",
       "      <td>January 2011</td>\n",
       "      <td>False</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Day Day_of_Week  Weekday  Weekday2  Month MonthName Monthshort  \\\n",
       "0  2010-12-29   29   Wednesday        2         4     12  December        Dec   \n",
       "1  2010-12-30   30    Thursday        3         5     12  December        Dec   \n",
       "2  2010-12-31   31      Friday        4         6     12  December        Dec   \n",
       "3  2011-01-01    1    Saturday        5         7      1   January        Jan   \n",
       "4  2011-01-02    2      Sunday        6         1      1   January        Jan   \n",
       "\n",
       "   Quarter  Year Year_Month     Month_Year    EOM EndofQuarter    EOQ  \n",
       "0        4  2010    2010_12  December 2010  False   2010-12-31  False  \n",
       "1        4  2010    2010_12  December 2010  False   2010-12-31  False  \n",
       "2        4  2010    2010_12  December 2010   True   2010-12-31   True  \n",
       "3        1  2011    2011_01   January 2011  False   2011-03-31  False  \n",
       "4        1  2011    2011_01   January 2011  False   2011-03-31  False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def DimDate(start_date, end_date):\n",
    "    from pandas.tseries.offsets import MonthEnd, QuarterEnd\n",
    "    \n",
    "    # Create base DataFrame with dates\n",
    "    dim_date = pd.DataFrame({\"Date\": pd.date_range(start=f'{start_date}', end=f'{end_date}', freq='D')})\n",
    "    \n",
    "    # Function to get end of month\n",
    "    def get_end_of_month(pd_date):\n",
    "        if pd_date.is_month_end == True:\n",
    "            return pd_date\n",
    "        else:\n",
    "            return pd_date + MonthEnd(1)\n",
    "    \n",
    "    # Function to get end of quarter\n",
    "    def get_end_of_quarter(pd_date):\n",
    "        if pd_date.is_quarter_end == True:\n",
    "            return pd_date\n",
    "        else:\n",
    "            return pd_date + QuarterEnd(1)\n",
    "    \n",
    "    # Add calendar-related fields\n",
    "    dim_date[\"Day\"] = dim_date[\"Date\"].dt.day\n",
    "    dim_date[\"Day_of_Week\"] = dim_date[\"Date\"].dt.day_name()\n",
    "    dim_date[\"Weekday\"] = dim_date[\"Date\"].dt.weekday\n",
    "    dim_date[\"Weekday2\"] = dim_date[\"Date\"].dt.dayofweek.apply(lambda x: ((x + 1) % 7) + 1)\n",
    "    dim_date[\"Month\"] = dim_date[\"Date\"].dt.month\n",
    "    dim_date[\"MonthName\"] = dim_date[\"Date\"].dt.month_name()\n",
    "    dim_date[\"Monthshort\"] = dim_date[\"Date\"].dt.strftime(\"%b\")\n",
    "    dim_date[\"Quarter\"] = dim_date[\"Date\"].dt.quarter\n",
    "    dim_date[\"Year\"] = dim_date[\"Date\"].dt.year\n",
    "    dim_date[\"Year_Month\"] = dim_date[\"Date\"].dt.strftime(\"%Y_%m\")\n",
    "    dim_date[\"Month_Year\"] = dim_date[\"Date\"].dt.strftime(\"%B %Y\") \n",
    "    dim_date[\"EOM\"] = dim_date[\"Date\"].dt.is_month_end\n",
    "    dim_date[\"EndofQuarter\"] = dim_date[\"Date\"].apply(get_end_of_quarter).dt.date\n",
    "    dim_date[\"EOQ\"] = dim_date[\"Date\"].dt.is_quarter_end\n",
    "    dim_date[\"Date\"] = dim_date[\"Date\"].dt.date # converting the actual datetime series to just date \n",
    "\n",
    "    \n",
    "    return dim_date\n",
    "\n",
    "dim_date = DimDate(start_date, end_date)\n",
    "dim_date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b1e3b6-09cd-4651-b9ad-0fa26a82b8e1",
   "metadata": {},
   "source": [
    "#### QUick checkpoint \n",
    "- check the schema of all the dataframes at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3773e6-f3c5-4cdd-8040-2d8a21c698ec",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:29.7335805Z",
       "execution_start_time": "2025-10-23T17:29:28.9591456Z",
       "normalized_state": "finished",
       "parent_msg_id": "12bca5e4-d935-4405-aa37-899ecbecb399",
       "queued_time": "2025-10-23T17:29:18.0730142Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (20230707.2238)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"949pt\" height=\"277pt\"\n",
       " viewBox=\"0.00 0.00 949.25 277.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 273)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-273 945.25,-273 945.25,4 -4,4\"/>\n",
       "<!-- crm_cust_info -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>crm_cust_info</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-55.62 0,-213.38 137.5,-213.38 137.5,-55.62 0,-55.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"68.75\" y=\"-196.07\" font-family=\"Times,serif\" font-size=\"14.00\">crm_cust_info</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-189.62 137.5,-189.62\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-172.32\" font-family=\"Times,serif\" font-size=\"14.00\">cst_id</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-156.57\" font-family=\"Times,serif\" font-size=\"14.00\">cst_key</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-140.82\" font-family=\"Times,serif\" font-size=\"14.00\">cst_firstname</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-125.08\" font-family=\"Times,serif\" font-size=\"14.00\">cst_lastname</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-109.33\" font-family=\"Times,serif\" font-size=\"14.00\">cst_marital_status</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-93.58\" font-family=\"Times,serif\" font-size=\"14.00\">cst_gndr</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-77.83\" font-family=\"Times,serif\" font-size=\"14.00\">cst_create_date</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-62.08\" font-family=\"Times,serif\" font-size=\"14.00\">dwh_create_date</text>\n",
       "</g>\n",
       "<!-- crm_prd_info -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>crm_prd_info</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"155.12,-47.75 155.12,-221.25 272.38,-221.25 272.38,-47.75 155.12,-47.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.75\" y=\"-203.95\" font-family=\"Times,serif\" font-size=\"14.00\">crm_prd_info</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"155.12,-197.5 272.38,-197.5\"/>\n",
       "<text text-anchor=\"start\" x=\"163.12\" y=\"-180.2\" font-family=\"Times,serif\" font-size=\"14.00\">prd_id</text>\n",
       "<text text-anchor=\"start\" x=\"163.12\" y=\"-164.45\" font-family=\"Times,serif\" font-size=\"14.00\">prd_key</text>\n",
       "<text text-anchor=\"start\" x=\"163.12\" y=\"-148.7\" font-family=\"Times,serif\" font-size=\"14.00\">prd_nm</text>\n",
       "<text text-anchor=\"start\" x=\"163.12\" y=\"-132.95\" font-family=\"Times,serif\" font-size=\"14.00\">prd_cost</text>\n",
       "<text text-anchor=\"start\" x=\"163.12\" y=\"-117.2\" font-family=\"Times,serif\" font-size=\"14.00\">prd_line</text>\n",
       "<text text-anchor=\"start\" x=\"163.12\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">prd_start_dt</text>\n",
       "<text text-anchor=\"start\" x=\"163.12\" y=\"-85.7\" font-family=\"Times,serif\" font-size=\"14.00\">prd_end_dt</text>\n",
       "<text text-anchor=\"start\" x=\"163.12\" y=\"-69.95\" font-family=\"Times,serif\" font-size=\"14.00\">cat_id</text>\n",
       "<text text-anchor=\"start\" x=\"163.12\" y=\"-54.2\" font-family=\"Times,serif\" font-size=\"14.00\">dwh_create_date</text>\n",
       "</g>\n",
       "<!-- crm_sales_details -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>crm_sales_details</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"290.38,-39.88 290.38,-229.12 421.12,-229.12 421.12,-39.88 290.38,-39.88\"/>\n",
       "<text text-anchor=\"middle\" x=\"355.75\" y=\"-211.82\" font-family=\"Times,serif\" font-size=\"14.00\">crm_sales_details</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"290.38,-205.38 421.12,-205.38\"/>\n",
       "<text text-anchor=\"start\" x=\"298.38\" y=\"-188.07\" font-family=\"Times,serif\" font-size=\"14.00\">sls_ord_num</text>\n",
       "<text text-anchor=\"start\" x=\"298.38\" y=\"-172.32\" font-family=\"Times,serif\" font-size=\"14.00\">sls_prd_key</text>\n",
       "<text text-anchor=\"start\" x=\"298.38\" y=\"-156.57\" font-family=\"Times,serif\" font-size=\"14.00\">sls_cust_id</text>\n",
       "<text text-anchor=\"start\" x=\"298.38\" y=\"-140.82\" font-family=\"Times,serif\" font-size=\"14.00\">sls_order_dt</text>\n",
       "<text text-anchor=\"start\" x=\"298.38\" y=\"-125.08\" font-family=\"Times,serif\" font-size=\"14.00\">sls_ship_dt</text>\n",
       "<text text-anchor=\"start\" x=\"298.38\" y=\"-109.33\" font-family=\"Times,serif\" font-size=\"14.00\">sls_due_dt</text>\n",
       "<text text-anchor=\"start\" x=\"298.38\" y=\"-93.58\" font-family=\"Times,serif\" font-size=\"14.00\">sls_sales</text>\n",
       "<text text-anchor=\"start\" x=\"298.38\" y=\"-77.83\" font-family=\"Times,serif\" font-size=\"14.00\">sls_quantity</text>\n",
       "<text text-anchor=\"start\" x=\"298.38\" y=\"-62.08\" font-family=\"Times,serif\" font-size=\"14.00\">sls_price</text>\n",
       "<text text-anchor=\"start\" x=\"298.38\" y=\"-46.33\" font-family=\"Times,serif\" font-size=\"14.00\">dwh_create_date</text>\n",
       "</g>\n",
       "<!-- erp_loc_a101 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>erp_loc_a101</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"439.12,-95 439.12,-174 556.38,-174 556.38,-95 439.12,-95\"/>\n",
       "<text text-anchor=\"middle\" x=\"497.75\" y=\"-156.7\" font-family=\"Times,serif\" font-size=\"14.00\">erp_loc_a101</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"439.12,-150.25 556.38,-150.25\"/>\n",
       "<text text-anchor=\"start\" x=\"447.12\" y=\"-132.95\" font-family=\"Times,serif\" font-size=\"14.00\">cid</text>\n",
       "<text text-anchor=\"start\" x=\"447.12\" y=\"-117.2\" font-family=\"Times,serif\" font-size=\"14.00\">cntry</text>\n",
       "<text text-anchor=\"start\" x=\"447.12\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">dwh_create_date</text>\n",
       "</g>\n",
       "<!-- erp_cust_az12 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>erp_cust_az12</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"574.12,-87.12 574.12,-181.88 691.38,-181.88 691.38,-87.12 574.12,-87.12\"/>\n",
       "<text text-anchor=\"middle\" x=\"632.75\" y=\"-164.57\" font-family=\"Times,serif\" font-size=\"14.00\">erp_cust_az12</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"574.12,-158.12 691.38,-158.12\"/>\n",
       "<text text-anchor=\"start\" x=\"582.12\" y=\"-140.82\" font-family=\"Times,serif\" font-size=\"14.00\">cid</text>\n",
       "<text text-anchor=\"start\" x=\"582.12\" y=\"-125.08\" font-family=\"Times,serif\" font-size=\"14.00\">bdate</text>\n",
       "<text text-anchor=\"start\" x=\"582.12\" y=\"-109.33\" font-family=\"Times,serif\" font-size=\"14.00\">gen</text>\n",
       "<text text-anchor=\"start\" x=\"582.12\" y=\"-93.58\" font-family=\"Times,serif\" font-size=\"14.00\">dwh_create_date</text>\n",
       "</g>\n",
       "<!-- erp_px_cat_g1v2 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>erp_px_cat_g1v2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"709.12,-79.25 709.12,-189.75 826.38,-189.75 826.38,-79.25 709.12,-79.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"767.75\" y=\"-172.45\" font-family=\"Times,serif\" font-size=\"14.00\">erp_px_cat_g1v2</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"709.12,-166 826.38,-166\"/>\n",
       "<text text-anchor=\"start\" x=\"717.12\" y=\"-148.7\" font-family=\"Times,serif\" font-size=\"14.00\">id</text>\n",
       "<text text-anchor=\"start\" x=\"717.12\" y=\"-132.95\" font-family=\"Times,serif\" font-size=\"14.00\">cat</text>\n",
       "<text text-anchor=\"start\" x=\"717.12\" y=\"-117.2\" font-family=\"Times,serif\" font-size=\"14.00\">subcat</text>\n",
       "<text text-anchor=\"start\" x=\"717.12\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">maintenance</text>\n",
       "<text text-anchor=\"start\" x=\"717.12\" y=\"-85.7\" font-family=\"Times,serif\" font-size=\"14.00\">dwh_create_date</text>\n",
       "</g>\n",
       "<!-- dim_date -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>dim_date</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"844.25,-0.5 844.25,-268.5 941.25,-268.5 941.25,-0.5 844.25,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"892.75\" y=\"-251.2\" font-family=\"Times,serif\" font-size=\"14.00\">dim_date</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"844.25,-244.75 941.25,-244.75\"/>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-227.45\" font-family=\"Times,serif\" font-size=\"14.00\">Date</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-211.7\" font-family=\"Times,serif\" font-size=\"14.00\">Day</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-195.95\" font-family=\"Times,serif\" font-size=\"14.00\">Day_of_Week</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-180.2\" font-family=\"Times,serif\" font-size=\"14.00\">Weekday</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-164.45\" font-family=\"Times,serif\" font-size=\"14.00\">Weekday2</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-148.7\" font-family=\"Times,serif\" font-size=\"14.00\">Month</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-132.95\" font-family=\"Times,serif\" font-size=\"14.00\">MonthName</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-117.2\" font-family=\"Times,serif\" font-size=\"14.00\">Monthshort</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">Quarter</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-85.7\" font-family=\"Times,serif\" font-size=\"14.00\">Year</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-69.95\" font-family=\"Times,serif\" font-size=\"14.00\">Year_Month</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-54.2\" font-family=\"Times,serif\" font-size=\"14.00\">Month_Year</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-38.45\" font-family=\"Times,serif\" font-size=\"14.00\">EOM</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-22.7\" font-family=\"Times,serif\" font-size=\"14.00\">EndofQuarter</text>\n",
       "<text text-anchor=\"start\" x=\"852.25\" y=\"-6.95\" font-family=\"Times,serif\" font-size=\"14.00\">EOQ</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fb8db3c3590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs_info = {}\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame) and not name.startswith('_'):\n",
    "        dfs_info[name] = list(obj.columns)\n",
    "\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='Entity Diagram')\n",
    "\n",
    "# Add each table as a node\n",
    "for table, columns in dfs_info.items():\n",
    "    # Create a record-style label: {TableName|col1\\l col2\\l ...}\n",
    "    label = \"{\" + table + \"|\" + \"\\\\l\".join(columns) + \"\\\\l}\"\n",
    "    dot.node(table, label=label, shape='record')\n",
    "\n",
    "IPython.display.display(dot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffde854-4cd7-465b-80d2-96962b9dd59f",
   "metadata": {},
   "source": [
    "## Enriching the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5aa617-8911-4de5-8221-362b60136b27",
   "metadata": {},
   "source": [
    "##### Add a City column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290de4c3-e05c-4783-a00d-3545712ebde4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:30.1166406Z",
       "execution_start_time": "2025-10-23T17:29:29.7348877Z",
       "normalized_state": "finished",
       "parent_msg_id": "257c4ab7-d66e-4564-a609-43d6006b10a5",
       "queued_time": "2025-10-23T17:29:18.3988475Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse-jupyter.display-view+json": {
       "isSummary": false,
       "language": "python",
       "table": {
        "rows": [
         {
          "0": "United States",
          "1": "New York",
          "2": "1121"
         },
         {
          "0": "United States",
          "1": "Los Angeles",
          "2": "1036"
         },
         {
          "0": "Australia",
          "1": "Sydney",
          "2": "917"
         },
         {
          "0": "United States",
          "1": "Chicago",
          "2": "898"
         },
         {
          "0": "United States",
          "1": "Houston",
          "2": "768"
         },
         {
          "0": "Australia",
          "1": "Melbourne",
          "2": "751"
         },
         {
          "0": "United States",
          "1": "Phoenix",
          "2": "690"
         },
         {
          "0": "Australia",
          "1": "Brisbane",
          "2": "594"
         },
         {
          "0": "United States",
          "1": "Philadelphia",
          "2": "591"
         },
         {
          "0": "France",
          "1": "Paris",
          "2": "580"
         },
         {
          "0": "United States",
          "1": "San Antonio",
          "2": "536"
         },
         {
          "0": "United Kingdom",
          "1": "London",
          "2": "510"
         },
         {
          "0": "United States",
          "1": "San Diego",
          "2": "436"
         },
         {
          "0": "Australia",
          "1": "Perth",
          "2": "393"
         },
         {
          "0": "United States",
          "1": "Dallas",
          "2": "382"
         },
         {
          "0": "Canada",
          "1": "Montreal",
          "2": "330"
         },
         {
          "0": "United Kingdom",
          "1": "Birmingham",
          "2": "326"
         },
         {
          "0": "Germany",
          "1": "Berlin",
          "2": "306"
         },
         {
          "0": "Canada",
          "1": "Toronto",
          "2": "304"
         },
         {
          "0": "United States",
          "1": "San Jose",
          "2": "298"
         },
         {
          "0": "Germany",
          "1": "Hamburg",
          "2": "296"
         },
         {
          "0": "France",
          "1": "Marseille",
          "2": "285"
         },
         {
          "0": "Australia",
          "1": "Adelaide",
          "2": "258"
         },
         {
          "0": "Germany",
          "1": "Munich",
          "2": "241"
         },
         {
          "0": "United Kingdom",
          "1": "Manchester",
          "2": "218"
         },
         {
          "0": "United States",
          "1": "Austin",
          "2": "216"
         },
         {
          "0": "Germany",
          "1": "Cologne",
          "2": "192"
         },
         {
          "0": "Canada",
          "1": "Calgary",
          "2": "175"
         },
         {
          "0": "Canada",
          "1": "Ottawa",
          "2": "169"
         },
         {
          "0": "France",
          "1": "Lyon",
          "2": "167"
         },
         {
          "0": "Germany",
          "1": "Frankfurt",
          "2": "164"
         },
         {
          "0": "France",
          "1": "Toulouse",
          "2": "160"
         },
         {
          "0": "Australia",
          "1": "Gold Coast",
          "2": "159"
         },
         {
          "0": "United States",
          "1": "Columbus",
          "2": "159"
         },
         {
          "0": "United States",
          "1": "Jacksonville",
          "2": "156"
         },
         {
          "0": "Australia",
          "1": "Canberra",
          "2": "155"
         },
         {
          "0": "United Kingdom",
          "1": "Glasgow",
          "2": "155"
         },
         {
          "0": "United States",
          "1": "Fort Worth",
          "2": "152"
         },
         {
          "0": "United Kingdom",
          "1": "Leeds",
          "2": "149"
         },
         {
          "0": "Germany",
          "1": "Stuttgart",
          "2": "138"
         },
         {
          "0": "United Kingdom",
          "1": "Liverpool",
          "2": "135"
         },
         {
          "0": "Canada",
          "1": "Edmonton",
          "2": "132"
         },
         {
          "0": "France",
          "1": "Nice",
          "2": "129"
         },
         {
          "0": "Australia",
          "1": "Newcastle",
          "2": "124"
         },
         {
          "0": "France",
          "1": "Nantes",
          "2": "114"
         },
         {
          "0": "Canada",
          "1": "Mississauga",
          "2": "111"
         },
         {
          "0": "Germany",
          "1": "Dortmund",
          "2": "106"
         },
         {
          "0": "Germany",
          "1": "DÃ¼sseldorf",
          "2": "105"
         },
         {
          "0": "United Kingdom",
          "1": "Bristol",
          "2": "101"
         },
         {
          "0": "France",
          "1": "Montpellier",
          "2": "98"
         },
         {
          "0": "United States",
          "1": "Charlotte",
          "2": "97"
         },
         {
          "0": "Canada",
          "1": "Vancouver",
          "2": "94"
         },
         {
          "0": "Canada",
          "1": "Brampton",
          "2": "82"
         },
         {
          "0": "Australia",
          "1": "Hobart",
          "2": "81"
         },
         {
          "0": "United Kingdom",
          "1": "Sheffield",
          "2": "77"
         },
         {
          "0": "Germany",
          "1": "Essen",
          "2": "75"
         },
         {
          "0": "United Kingdom",
          "1": "Edinburgh",
          "2": "75"
         },
         {
          "0": "Canada",
          "1": "Hamilton",
          "2": "72"
         },
         {
          "0": "France",
          "1": "Strasbourg",
          "2": "68"
         },
         {
          "0": "France",
          "1": "Bordeaux",
          "2": "58"
         },
         {
          "0": "United Kingdom",
          "1": "Kingston upon Hull",
          "2": "57"
         },
         {
          "0": "Canada",
          "1": "Surrey",
          "2": "55"
         },
         {
          "0": "Germany",
          "1": "Leipzig",
          "2": "53"
         },
         {
          "0": "Australia",
          "1": "Townsville",
          "2": "48"
         },
         {
          "0": "United Kingdom",
          "1": "Bradford",
          "2": "47"
         },
         {
          "0": "France",
          "1": "Reims",
          "2": "47"
         },
         {
          "0": "Germany",
          "1": "Bremen",
          "2": "47"
         },
         {
          "0": "France",
          "1": "Rennes",
          "2": "46"
         },
         {
          "0": "France",
          "1": "Lille",
          "2": "43"
         },
         {
          "0": "United Kingdom",
          "1": "Coventry",
          "2": "39"
         },
         {
          "0": "United Kingdom",
          "1": "Leicester",
          "2": "39"
         },
         {
          "0": "Germany",
          "1": "Hanover",
          "2": "37"
         },
         {
          "0": "Australia",
          "1": "Wollongong",
          "2": "35"
         },
         {
          "0": "Australia",
          "1": "Geelong",
          "2": "35"
         },
         {
          "0": "Germany",
          "1": "Nuremberg",
          "2": "34"
         },
         {
          "0": "Australia",
          "1": "Cairns",
          "2": "34"
         },
         {
          "0": "Australia",
          "1": "Darwin",
          "2": "33"
         },
         {
          "0": "United Kingdom",
          "1": "Belfast",
          "2": "31"
         },
         {
          "0": "Australia",
          "1": "Toowoomba",
          "2": "30"
         },
         {
          "0": "Canada",
          "1": "Laval",
          "2": "28"
         },
         {
          "0": "Germany",
          "1": "Dresden",
          "2": "28"
         },
         {
          "0": "France",
          "1": "Le Havre",
          "2": "26"
         },
         {
          "0": "Canada",
          "1": "Quebec City",
          "2": "25"
         },
         {
          "0": "Canada",
          "1": "Halifax",
          "2": "24"
         },
         {
          "0": "United Kingdom",
          "1": "Cardiff",
          "2": "23"
         },
         {
          "0": "France",
          "1": "Toulon",
          "2": "20"
         },
         {
          "0": "Canada",
          "1": "London",
          "2": "17"
         },
         {
          "0": "Canada",
          "1": "Markham",
          "2": "13"
         },
         {
          "0": "France",
          "1": "Saint-Ã‰tienne",
          "2": "13"
         },
         {
          "0": "Germany",
          "1": "Duisburg",
          "2": "12"
         }
        ],
        "schema": [
         {
          "key": "0",
          "name": "cntry",
          "type": "string"
         },
         {
          "key": "1",
          "name": "city",
          "type": "string"
         },
         {
          "key": "2",
          "name": "frequency",
          "type": "bigint"
         }
        ],
        "truncated": false
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "city_map = {\n",
    "    'Australia': ([\n",
    "        'Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide', \n",
    "        'Gold Coast', 'Canberra', 'Newcastle', 'Hobart', 'Wollongong',\n",
    "        'Geelong', 'Townsville', 'Cairns', 'Toowoomba', 'Darwin'\n",
    "    ], [\n",
    "        0.25, 0.22, 0.18, 0.12, 0.08, \n",
    "        0.05, 0.04, 0.03, 0.02, 0.01,\n",
    "        0.01, 0.01, 0.01, 0.01, 0.01\n",
    "    ]),\n",
    "    'United States': ([\n",
    "        'New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix',\n",
    "        'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose',\n",
    "        'Austin', 'Jacksonville', 'Fort Worth', 'Columbus', 'Charlotte'\n",
    "    ], [\n",
    "        0.15, 0.13, 0.12, 0.10, 0.09,\n",
    "        0.08, 0.07, 0.06, 0.05, 0.04,\n",
    "        0.03, 0.02, 0.02, 0.02, 0.01\n",
    "    ]),\n",
    "    'Canada': ([\n",
    "        'Toronto', 'Montreal', 'Calgary', 'Ottawa', 'Edmonton',\n",
    "        'Mississauga', 'Vancouver', 'Brampton', 'Hamilton', 'Surrey',\n",
    "        'Quebec City', 'Laval', 'Halifax', 'London', 'Markham'\n",
    "    ], [\n",
    "        0.20, 0.18, 0.12, 0.10, 0.08,\n",
    "        0.07, 0.06, 0.05, 0.04, 0.03,\n",
    "        0.02, 0.02, 0.02, 0.01, 0.01\n",
    "    ]),\n",
    "    'Germany': ([\n",
    "        'Berlin', 'Hamburg', 'Munich', 'Cologne', 'Frankfurt',\n",
    "        'Stuttgart', 'DÃ¼sseldorf', 'Dortmund', 'Essen', 'Leipzig',\n",
    "        'Bremen', 'Dresden', 'Hanover', 'Nuremberg', 'Duisburg'\n",
    "    ], [\n",
    "        0.18, 0.16, 0.15, 0.10, 0.09,\n",
    "        0.08, 0.07, 0.06, 0.05, 0.04,\n",
    "        0.03, 0.02, 0.02, 0.02, 0.01\n",
    "    ]),\n",
    "    'United Kingdom': ([\n",
    "        'London', 'Birmingham', 'Manchester', 'Glasgow', 'Leeds',\n",
    "        'Liverpool', 'Bristol', 'Sheffield', 'Edinburgh', 'Leicester',\n",
    "        'Coventry', 'Kingston upon Hull', 'Bradford', 'Cardiff', 'Belfast'\n",
    "    ], [\n",
    "        0.25, 0.15, 0.10, 0.08, 0.07,\n",
    "        0.06, 0.05, 0.04, 0.03, 0.02,\n",
    "        0.02, 0.02, 0.02, 0.01, 0.01\n",
    "    ]),\n",
    "    'France': ([\n",
    "        'Paris', 'Marseille', 'Lyon', 'Toulouse', 'Nice',\n",
    "        'Nantes', 'Montpellier', 'Strasbourg', 'Bordeaux', 'Lille',\n",
    "        'Rennes', 'Reims', 'Le Havre', 'Saint-Ã‰tienne', 'Toulon'\n",
    "    ], [\n",
    "        0.30, 0.15, 0.10, 0.08, 0.07,\n",
    "        0.06, 0.05, 0.04, 0.03, 0.02,\n",
    "        0.02, 0.02, 0.02, 0.01, 0.01\n",
    "    ])\n",
    "}\n",
    "\n",
    "def assign_city(country):\n",
    "    cities, weights = city_map.get(country, ([\"Unknown\"], [1]))\n",
    "    return random.choices(cities, weights=weights, k=1)[0]\n",
    "\n",
    "erp_loc_a101[\"city\"] = erp_loc_a101[\"cntry\"].apply(assign_city)\n",
    "display(erp_loc_a101[[\"cntry\",\"city\"]].value_counts().reset_index(name=\"frequency\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bce8cf-e766-4db5-85bd-f2776713e219",
   "metadata": {},
   "source": [
    "## Load Dataframes into silver layer delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57943a1d-3ac4-4c2e-a0aa-322e6cd2fb19",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:30.474509Z",
       "execution_start_time": "2025-10-23T17:29:30.1178866Z",
       "normalized_state": "finished",
       "parent_msg_id": "c88c4b32-5d3b-4224-9d5b-c1a3d0050318",
       "queued_time": "2025-10-23T17:29:18.663802Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_tables():\n",
    "    # Write crm_cust_info to silver layer delta table\n",
    "    print(\"\\nWriting crm_cust_info dataframe to silver delta table.................\")\n",
    "    pl.from_pandas(crm_cust_info).write_delta(f\"{abfs_path}/Tables/crm_cust_info\", mode=\"append\")\n",
    "    print(\"Silver layer table 'crm_cust_info' loaded successfully\")\n",
    "\n",
    "    # Write crm_prd_info to Silver Layer Delta Table\n",
    "    print(\"\\nWriting crm_prd_info dataframe into the silver layer delta table.................\")\n",
    "    pl.from_pandas(crm_prd_info).write_delta(f\"{abfs_path}/Tables/crm_prd_info\", mode=\"append\")\n",
    "    print(\"Silver layer table 'crm_prd_info' loaded successfully\")  \n",
    "\n",
    "    #Writing the crm_sales_detail dataframe to silver layer Delta Table\n",
    "    print(\"\\nWriting crm_sales_details dataframe into the silver layer delta table.................\")\n",
    "    pl.from_pandas(crm_sales_details).write_delta(f\"{abfs_path}/Tables/crm_sales_details\", mode=\"append\")\n",
    "    print(\"Silver layer table 'crm_sales_details' loaded successfully\")\n",
    "\n",
    "    #write erp_cust_az12 to a silver layer table\n",
    "    print(\"\\nlaading erp_cust_az12 into silver layer table.............\")\n",
    "    pl.from_pandas(erp_cust_az12).write_delta(f\"{abfs_path}/Tables/erp_cust_az12\", mode=\"append\")\n",
    "    print(\"Silver layer table 'erp_cust_az12' loaded successfully\")\n",
    "\n",
    "    #write the erp_loc_a101 dataset to a silver layer table\n",
    "    print(\"\\nlaading erp_loc_a101 dataframe into silver layer table.............\")\n",
    "    pl.from_pandas(erp_loc_a101).write_delta(f\"{abfs_path}/Tables/erp_loc_a101\", mode=\"overwrite\",delta_write_options={\"schema_mode\":\"overwrite\"})\n",
    "    print(\"Silver layer table 'erp_loc_a101' loaded successfully\")\n",
    "\n",
    "    #write the erp_px_cat_g1v2 dataset to a silver layer table\n",
    "    print(\"\\nlaading the erp_px_cat_g1v2 dataframe into silver layer table.............\")\n",
    "    pl.from_pandas(erp_px_cat_g1v2).write_delta(f\"{abfs_path}/Tables/erp_px_cat_g1v2\", mode=\"append\")\n",
    "    print(\"Silver layer table 'erp_px_cat_g1v2' loaded successfully\")\n",
    "\n",
    "    # Write the date dataframe to silver table \n",
    "    print(\"\\nlaading dim_date dataframe into silver layer table.............\")\n",
    "    pl.from_pandas(dim_date).write_delta(f\"{abfs_path}/Tables/dim_date\", mode=\"overwrite\", delta_write_options={\"schema_mode\":\"overwrite\"})\n",
    "    print(\"Silver layer table 'dim_date' loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac1f123b-0257-4478-be9b-dc599fe58faf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T17:29:35.8411342Z",
       "execution_start_time": "2025-10-23T17:29:30.4759132Z",
       "normalized_state": "finished",
       "parent_msg_id": "e1a7e429-c3b2-4fc9-bc51-bff036b79364",
       "queued_time": "2025-10-23T17:29:18.8923717Z",
       "session_id": "67bc7949-5c9e-4841-8e84-2aa7a0eae983",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing crm_cust_info dataframe to silver delta table.................\n",
      "Silver layer table 'crm_cust_info' loaded successfully\n",
      "\n",
      "Writing crm_prd_info dataframe into the silver layer delta table.................\n",
      "Silver layer table 'crm_prd_info' loaded successfully\n",
      "\n",
      "Writing crm_sales_details dataframe into the silver layer delta table.................\n",
      "Silver layer table 'crm_sales_details' loaded successfully\n",
      "\n",
      "laading erp_cust_az12 into silver layer table.............\n",
      "Silver layer table 'erp_cust_az12' loaded successfully\n",
      "\n",
      "laading erp_loc_a101 dataframe into silver layer table.............\n",
      "Silver layer table 'erp_loc_a101' loaded successfully\n",
      "\n",
      "laading the erp_px_cat_g1v2 dataframe into silver layer table.............\n",
      "Silver layer table 'erp_px_cat_g1v2' loaded successfully\n",
      "\n",
      "laading dim_date dataframe into silver layer table.............\n",
      "Silver layer table 'dim_date' loaded successfully\n"
     ]
    }
   ],
   "source": [
    "load_tables()"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
