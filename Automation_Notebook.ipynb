{"cells":[{"cell_type":"code","source":["%pip install semantic-link==0.11.1 semantic-link-labs==0.11.2  > /dev/null 2>&1\n","import pandas as pd\n","import requests \n","import json\n","import base64\n","import sempy_labs as labs\n","import sempy_labs.report as rep \n","import sempy.fabric as semfabric\n","from urllib.parse import urlparse, unquote\n","from typing import Union, Optional\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"01ac2e86-19af-46d8-bb23-5c87b8cf0bf0"},{"cell_type":"code","source":["## You can change these as you like\n","lakehouse_name = 'Baraa_LH'\n","warehouse_name = 'Baraa_WH'\n","semantic_model_name = 'Baraa_SMM'\n","report_name = 'Baraa_Report'\n","gold_notebook_name = 'gold_layer_processing_notebook'\n","silver_notebook_name = 'silver_layer_processing_notebook'\n","timezone = \"Africa/Lagos\"\n","\n","## Don't Change these ⚠️⚠️⚠️⚠️⚠️⚠️⚠️\n","crm_data_relative_path = \"datasets/source_crm\"\n","erp_data_relative_path = \"datasets/source_erp\"\n","\n","workspace_name=notebookutils.runtime.context['currentWorkspaceName']\n","workspace_id = notebookutils.runtime.context['currentWorkspaceId']\n","\n","create_semantic_model_uri = f\"v1/workspaces/{workspace_id}/semanticModels\"\n","semantic_model_github_url = \"https://github.com/maleek004/baraaE2E/tree/main/FakemazonSMM.SemanticModel\"\n","semantic_model_parts = ''\n","semantic_model_request_body= ''\n","semantic_model_id = ''\n","\n","create_report_uri = f\"v1/workspaces/{workspace_id}/reports\"\n","report_github_folder = 'https://github.com/maleek004/baraaE2E/tree/main/auto%20generated%20report.Report'\n","report_parts = ''\n","create_report_request_body = ''\n","report_id =''\n","\n","silver_notebook_url = 'https://raw.githubusercontent.com/maleek004/auto-fabric-setup-baraa/refs/heads/main/Create_silver_layer_Notebook.ipynb'\n","gold_notebook_url= 'https://raw.githubusercontent.com/maleek004/auto-fabric-setup-baraa/refs/heads/main/Load_gold_layer.ipynb'\n","\n","crm_dataset_urls = {\"cust_info.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_crm/cust_info.csv\"\n","                    ,\"prd_info.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_crm/prd_info.csv\"\n","                    ,\"sales_details.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_crm/sales_details.csv\"}\n","\n","erp_dataset_urls = {\"CUST_AZ12.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_erp/CUST_AZ12.csv\"\n","                    ,\"LOC_A101.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_erp/LOC_A101.csv\"\n","                    ,\"PX_CAT_G1V2.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_erp/PX_CAT_G1V2.csv\"}"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"e7af4004-a6d3-4711-a00f-f385c704d1ef"},{"cell_type":"markdown","source":["## Functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64d3b43d-4e29-466b-90e5-5f3a8f763a7a"},{"cell_type":"code","source":["####################### FOR SENDING POST REQUESTS TO FABRIC REST API (WITHOUT BEARER TOKEN)\n","def fabriclient_post(url, request_body):\n","\n","    client = semfabric.FabricRestClient()\n","    \n","    response = client.request(method = \"POST\", path_or_url=url, lro_wait=True, json = request_body)\n","    print(response.status_code)\n","    print(response.text)\n","    response.raise_for_status()\n","    \n","    response_json = response.json()\n","    return response_json.get('id')\n","\n","####################### THIS FUNCTION EXTRACTS THE DEFINITION PART OF AN ITEM SYNCED TO GITHUB AND RETURNS THE FULL LIST WITH BASE64 PAYLOAD\n","\n","def get_definition_parts_from_github(github_folder_url, isReport=False, report_pbir_definition: Optional[Union[dict, str, bytes]] = None):\n","    \"\"\"\n","    Given a GitHub folder URL, recursively retrieves all files under it\n","    and returns a 'parts' list formatted like:\n","    [\n","        {\n","            \"path\": \"definition.pbism\",\n","            \"payload\": \"<base64 string>\",\n","            \"payloadType\": \"InlineBase64\"\n","        },\n","        ...\n","    ]\n","\n","    Parameters:\n","    ----------\n","    github_folder_url : str\n","        The GitHub folder URL to process.\n","    isReport : bool, optional\n","        Set to True if the folder represents a Power BI report.\n","        If True, `report_pbir_definition` must be provided.\n","    report_pbir_definition : bytes, optional\n","        The customized definition.pbir file to use (required if isReport=True).\n","\n","    Raises:\n","    -------\n","    ValueError\n","        If isReport=True but report_pbir_definition is not provided.\n","    \"\"\"\n","\n","    # ---  Validate parameters ---\n","    if isReport and report_pbir_definition is None:\n","        raise ValueError(\n","            \"Since you are trying to build a report, you must provide a custom 'definition.pbir' \"\n","            \"file that includes a 'byConnection' dataReference and the target 'semanticmodelid'.\\n\"\n","            \"For more information, visit: \"\n","            \"https://learn.microsoft.com/en-us/power-bi/developer/projects/projects-report\"\n","            \"?tabs=v2%2Cdesktop#definitionpbir\"\n","        )\n","    # --- Helper: Parse the folder URL ---\n","    def parse_github_folder_url(url: str):\n","        parsed = urlparse(url)\n","        path_parts = parsed.path.strip(\"/\").split(\"/\")\n","\n","        if len(path_parts) < 2:\n","            raise ValueError(\"Invalid GitHub URL format.\")\n","\n","        owner = path_parts[0]\n","        repo = path_parts[1]\n","\n","        # Handle 'tree/main/...'\n","        if len(path_parts) > 3 and path_parts[2] in (\"tree\", \"blob\"):\n","            subpath = \"/\".join(path_parts[4:])\n","        else:\n","            subpath = \"/\".join(path_parts[2:])\n","\n","        return owner, repo, unquote(subpath)\n","\n","    # --- Helper: Recursively fetch all files and relative paths ---\n","    def get_all_files_with_relative_paths(owner: str, repo: str, base_path: str, current_path: str = None):\n","        if current_path is None:\n","            current_path = base_path\n","\n","        api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{current_path}\"\n","        response = requests.get(api_url)\n","        response.raise_for_status()\n","        items = response.json()\n","\n","        files = []\n","        for item in items:\n","            if item[\"type\"] == \"file\":\n","                relative_path = item[\"path\"].replace(base_path + \"/\", \"\")\n","                files.append({\n","                    \"raw_url\": item[\"download_url\"],\n","                    \"relative_path\": relative_path\n","                })\n","            elif item[\"type\"] == \"dir\":\n","                files.extend(get_all_files_with_relative_paths(owner, repo, base_path, item[\"path\"]))\n","        return files\n","\n","    # --- Helper: Download and encode file content to Base64 ---\n","    def download_and_encode_base64(url: str) -> str:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","        file_content = response.content  # .content was the safest option\n","        return base64.b64encode(file_content).decode(\"utf-8\")\n","\n","    # --- Main flow ---\n","    owner, repo, subpath = parse_github_folder_url(github_folder_url)\n","    files = get_all_files_with_relative_paths(owner, repo, subpath)\n","\n","    parts = []\n","    for file in files:\n","        if isReport and file['relative_path'] == 'definition.pbir':\n","            # Safely handle dicts, strings, or bytes\n","            if isinstance(report_pbir_definition, dict):\n","                report_pbir_definition_bytes = json.dumps(report_pbir_definition, indent=2).encode(\"utf-8\")\n","            elif isinstance(report_pbir_definition, str):\n","                report_pbir_definition_bytes = report_pbir_definition.encode(\"utf-8\")\n","            else:\n","                report_pbir_definition_bytes = report_pbir_definition  # assume bytes\n","            encoded = base64.b64encode(report_pbir_definition).decode(\"utf-8\")\n","        else:\n","            encoded = download_and_encode_base64(file[\"raw_url\"])\n","        parts.append({\n","            \"path\": file[\"relative_path\"],\n","            \"payload\": encoded,\n","            \"payloadType\": \"InlineBase64\"\n","        })\n","\n","    print(f\"✅ Retrieved {len(parts)} files from {repo}/{subpath}\")\n","    return parts\n","\n","##################### THIS FUNCTION DOWNLOADS DATASET TO LAKEHOUSE\n","def download_datasets(dataset_urls, save_dir=\".\"):\n","    for filename, url in dataset_urls.items():\n","        print(f\"Downloading {filename}...\")\n","        try:\n","            response = requests.get(url, timeout=15)\n","            response.raise_for_status()  # Raise exception for HTTP errors\n","            with open(f\"{save_dir}/{filename}\", \"wb\") as f:\n","                f.write(response.content)\n","            print(f\"✅ {filename} downloaded successfully.\")\n","        except requests.exceptions.RequestException as e:\n","            print(f\"❌ Failed to download {filename}: {e}\")\n","\n","################ download notebook from github and optionally attach it to a lakehouse\n","def import_notebook(notebook_import_name, githuburl, update_lakehouse=False , workspace_id=None, lakehouse_name=None ) -> str:\n","    # import notebook and return notebook_id\n","    result = labs.import_notebook_from_web( notebook_name=notebook_import_name, url=githuburl)\n","        \n","    # update the default lakehouse (only if allowed)\n","    if update_lakehouse:\n","        if not workspace_id or not lakehouse_name:\n","            raise ValueError(\"workspace_id and lakehouse_name must be provided when update_lakehouse=True\")\n","        notebookutils.notebook.updateDefinition(\n","            name=notebook_import_name, \n","            workspaceId=workspace_id, \n","            defaultLakehouse=lakehouse_name, \n","            defaultLakehouseWorkspace=workspace_id\n","        )\n","    \n","    # Return notebook id\n","    notebook_id = semfabric.resolve_item_id(item_name=notebook_import_name, type=\"Notebook\")\n","\n","    print(f\"notebookname: {notebook_import_name}, notebook_id: {notebook_id}\")\n","\n","    return notebook_id\n","\n","##########\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"f2422e7c-5748-400c-b9e8-58ceff385819"},{"cell_type":"markdown","source":["## Create Lakehouse\n","#### Created directories in the lakehouse\n","#### Mounted the file directory of the lakehouse so that we can write files to it directly\n","#### Downloaded files from Github into folders in the Lakehouse  "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cab20839-2b9a-472d-a6b3-47f2017835cc"},{"cell_type":"code","source":["# creating a new lakehouse where our data will be downladed will be downloaded into and will alsoe be used as the siver layer\n","\n","lakehouse = notebookutils.lakehouse.create(lakehouse_name)    \n","lakehouse_id = lakehouse['id']\n","abfs_path = lakehouse.get('properties',{}).get('abfsPath')\n","abfs_path"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"5e3ab6bc-b89e-4d65-bc92-f310638d70ae"},{"cell_type":"code","source":["# creating directories in our destination lakehouse (that this notebook is not attached to, hence the use of abfs_path) \n","notebookutils.fs.mkdirs(f'{abfs_path}/Files/{crm_data_relative_path}')\n","notebookutils.fs.mkdirs(f'{abfs_path}/Files/{erp_data_relative_path}')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"78bac567-be85-457b-81de-133f43ca3ca6"},{"cell_type":"code","source":["##  mount the Files section of the new lakehouse \n","files_directory = abfs_path + '/Files'\n","mount_point = \"/mnt/lakehouse/\" + lakehouse_name + \"/Files\"\n","notebookutils.fs.mount(files_directory, mount_point)\n","base_dir_local_path = notebookutils.fs.getMountPath(mount_point)\n","base_dir_local_path"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"e554749f-4533-49e8-9c03-d5093990570c"},{"cell_type":"code","source":["crm_data_full_local_path = f'{base_dir_local_path}/{crm_data_relative_path}'\n","erp_data_full_local_path = f'{base_dir_local_path}/{erp_data_relative_path}'\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"b781f4c2-bf2b-46b5-94f3-6429389c69e5"},{"cell_type":"code","source":["download_datasets(crm_dataset_urls, crm_data_full_local_path)\n","download_datasets(erp_dataset_urls, erp_data_full_local_path)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"50121c39-73c5-4b4c-ae6e-1b881dcb3cf4"},{"cell_type":"markdown","source":["## Create Warehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c11633c8-621d-4140-8599-aa50ef5650ec"},{"cell_type":"code","source":["#Creating a warehouse where our gold layer views and tables will be stored\n","url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/warehouses\"\n","request_body = {\"displayName\": warehouse_name}\n","\n","warehouse_id = fabriclient_post(url,request_body)\n","warehouse_id"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"7ca1f3a5-3afb-404d-9b59-d63b48600059"},{"cell_type":"markdown","source":["## Download and run transformation notebooks "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"79154a14-274f-4324-a12a-392cd7ad9015"},{"cell_type":"code","source":["#import notebooks and get Notebook Ids for the 2 notebooks to be used in subsequent steps\n","silver_layer_notebook_id = import_notebook(notebook_import_name=silver_notebook_name, githuburl=silver_notebook_url, update_lakehouse=True, workspace_id=workspace_id, lakehouse_name = lakehouse_name)\n","gold_layer_notebook_id = import_notebook(gold_notebook_name,gold_notebook_url)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"ef344463-e33e-4b72-8d42-9fa4c09ef0ba"},{"cell_type":"code","source":["DAG = {\n","    \"activities\": [\n","        {\n","            \"name\": silver_notebook_name, \n","            \"path\": silver_notebook_name, \n","            \"timeoutPerCellInSeconds\": 90,\n","            \"args\": {\"useRootDefaultLakehouse\": True, \"abfs_path\":abfs_path, \"workspaceId\": workspace_id , \"lakehouseId\": lakehouse_id , \"timezone\" : timezone}\n","        },\n","        {\n","            \"name\": gold_notebook_name,\n","            \"path\": gold_notebook_name,\n","            \"timeoutPerCellInSeconds\": 90,\n","            \"args\":{\"lakehouse_name\": lakehouse_name , \"warehouse_name\":warehouse_name},\n","            \"retry\": 2, ### because it takes a while for the SQL endpoint of the lakehouse to refresh, and we will be depending on it so the first try might fail...\n","            \"retryIntervalInSeconds\": 20,\n","            \"dependencies\": [silver_notebook_name]\n","        }\n","    ],\n","    \"timeoutInSeconds\": 480, # max 4 mins for the entire pipeline\n","}\n","\n","notebookutils.notebook.runMultiple(DAG, {\"displayDAGViaGraphviz\":True})"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"3b93bde9-750c-4871-8e65-5e49ec02478b"},{"cell_type":"code","source":["### Create semantic model from Fabric REST API and attach it to warehouse\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"63efa15d-41d6-4053-86ea-dbe03acd1644"},{"cell_type":"code","source":["semantic_model_parts = get_definition_parts_from_github(semantic_model_github_url)\n","semantic_model_request_body= {\n","                                \"displayName\": semantic_model_name,\n","                                \"description\": \"semantic model for E2E demo\",\n","                                \"definition\":{\n","                                        \"parts\": semantic_model_parts\n","                                } \n","                            }\n","\n","semantic_model_id = fabriclient_post(create_semantic_model_uri, semantic_model_request_body)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"e754d529-a2e0-4c31-a824-abfcfc0d5292"},{"cell_type":"code","source":["labs.directlake.update_direct_lake_model_connection(dataset = semantic_model_name,source= warehouse_name, source_type=\"Warehouse\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"a01f077c-42ca-4bc7-8757-9318fb0b9105"},{"cell_type":"markdown","source":["## Create report "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0492962d-0d58-402b-8ba6-104f0fd4fead"},{"cell_type":"code","source":["custom_pbir = {\n","    \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/item/report/definitionProperties/2.0.0/schema.json\",\n","    \"version\": \"4.0\",\n","    \"datasetReference\": {\n","        \"byConnection\": {\n","            \"connectionString\": f\"semanticmodelid={semantic_model_id}\"\n","        }\n","    }\n","}\n","\n","report_parts = get_definition_parts_from_github(report_github_folder, isReport=True , report_pbir_definition=custom_pbir)\n","create_report_request_body = {\n","        \"displayName\": report_name,\n","        \"description\": \"report created using Fabric REST API\",\n","        \"definition\" : {\n","            \"parts\": report_parts\n","            }\n","        }\n","report_id = fabriclient_post(create_report_uri, create_report_request_body)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"ca5e3f27-c067-49e7-ba88-c2c4317975ad"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"681992ae-2615-4cfc-a9fc-df3ba7e61207"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}