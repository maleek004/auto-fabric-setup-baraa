{"cells":[{"cell_type":"code","source":["%pip install semantic-link==0.11.1 semantic-link-labs==0.11.2  > /dev/null 2>&1\n","import pandas as pd\n","import requests \n","import json\n","import base64\n","import sempy_labs as labs\n","import sempy_labs.report as rep \n","import sempy.fabric as semfabric\n","from urllib.parse import urlparse, unquote\n","from typing import Union, Optional\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"d56c2484-15a2-4fde-9f2d-17ae4ef60e55"},{"cell_type":"code","source":["## You can change these as you like\n","lakehouse_name = 'Baraa_LH'\n","warehouse_name = 'Baraa_WH'\n","semantic_model_name = 'Baraa_SMM'\n","report_name = 'Baraa_Report'\n","gold_notebook_name = 'gold_layer_processing_notebook'\n","silver_notebook_name = 'silver_layer_processing_notebook'\n","timezone = \"Africa/Lagos\" # this is used for adding Data warehouse creation dates column in the transformation notebooks\n","\n","## Don't Change these ⚠️⚠️⚠️⚠️⚠️⚠️⚠️\n","crm_data_relative_path = \"datasets/source_crm\"\n","erp_data_relative_path = \"datasets/source_erp\"\n","\n","workspace_name=notebookutils.runtime.context['currentWorkspaceName']\n","workspace_id = notebookutils.runtime.context['currentWorkspaceId']\n","\n","create_semantic_model_uri = f\"v1/workspaces/{workspace_id}/semanticModels\"\n","semantic_model_github_url = \"https://github.com/maleek004/baraaE2E/tree/main/Baraa_DL_SMM.SemanticModel\"\n","semantic_model_parts = ''\n","semantic_model_request_body= ''\n","semantic_model_id = ''\n","\n","create_report_uri = f\"v1/workspaces/{workspace_id}/reports\"\n","report_github_folder = 'https://github.com/maleek004/baraaE2E/tree/main/Baraa_DL_Report.Report'\n","report_parts = ''\n","create_report_request_body = ''\n","report_id =''\n","\n","silver_notebook_url = 'https://raw.githubusercontent.com/maleek004/auto-fabric-setup-baraa/refs/heads/main/Create_silver_layer_Notebook.ipynb'\n","gold_notebook_url= 'https://raw.githubusercontent.com/maleek004/auto-fabric-setup-baraa/refs/heads/main/Load_gold_layer.ipynb'\n","\n","crm_dataset_urls = {\"cust_info.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_crm/cust_info.csv\"\n","                    ,\"prd_info.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_crm/prd_info.csv\"\n","                    ,\"sales_details.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_crm/sales_details.csv\"}\n","\n","erp_dataset_urls = {\"CUST_AZ12.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_erp/CUST_AZ12.csv\"\n","                    ,\"LOC_A101.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_erp/LOC_A101.csv\"\n","                    ,\"PX_CAT_G1V2.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_erp/PX_CAT_G1V2.csv\"}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"858da73a-b8ca-4db8-81fb-e41fc187182c","normalized_state":"finished","queued_time":"2025-11-08T16:00:56.7711518Z","session_start_time":null,"execution_start_time":"2025-11-08T16:02:13.5314717Z","execution_finish_time":"2025-11-08T16:02:13.8886072Z","parent_msg_id":"9b618d42-9807-4591-8a83-33bd70dd5cf3"},"text/plain":"StatementMeta(, 858da73a-b8ca-4db8-81fb-e41fc187182c, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"1cdc6f27-83da-4487-a73d-806d03a88133"},{"cell_type":"markdown","source":["## Functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"acf7612b-f33b-45f7-89ad-d64587c5b94f"},{"cell_type":"code","source":["####################### FOR SENDING POST REQUESTS TO FABRIC REST API (WITHOUT BEARER TOKEN)\n","def fabriclient_post(url, request_body):\n","\n","    client = semfabric.FabricRestClient()\n","    \n","    response = client.request(method = \"POST\", path_or_url=url, lro_wait=True, json = request_body)\n","    print(response.status_code)\n","    print(response.text)\n","    response.raise_for_status()\n","    \n","    response_json = response.json()\n","    return response_json.get('id')\n","\n","####################### THIS FUNCTION EXTRACTS THE DEFINITION PART OF AN ITEM SYNCED TO GITHUB AND RETURNS THE FULL LIST WITH BASE64 PAYLOAD\n","\n","def get_definition_parts_from_github(github_folder_url, isReport=False, report_pbir_definition: Optional[Union[dict, str, bytes]] = None):\n","    \"\"\"\n","    Given a GitHub folder URL, recursively retrieves all files under it\n","    and returns a 'parts' list formatted like:\n","    [\n","        {\n","            \"path\": \"definition.pbism\",\n","            \"payload\": \"<base64 string>\",\n","            \"payloadType\": \"InlineBase64\"\n","        },\n","        ...\n","    ]\n","\n","    Parameters:\n","    ----------\n","    github_folder_url : str\n","        The GitHub folder URL to process.\n","    isReport : bool, optional\n","        Set to True if the folder represents a Power BI report.\n","        If True, `report_pbir_definition` must be provided.\n","    report_pbir_definition : bytes, optional\n","        The customized definition.pbir file to use (required if isReport=True).\n","\n","    Raises:\n","    -------\n","    ValueError\n","        If isReport=True but report_pbir_definition is not provided.\n","    \"\"\"\n","\n","    # ---  Validate parameters ---\n","    if isReport and report_pbir_definition is None:\n","        raise ValueError(\n","            \"Since you are trying to build a report, you must provide a custom 'definition.pbir' \"\n","            \"file that includes a 'byConnection' dataReference and the target 'semanticmodelid'.\\n\"\n","            \"For more information, visit: \"\n","            \"https://learn.microsoft.com/en-us/power-bi/developer/projects/projects-report\"\n","            \"?tabs=v2%2Cdesktop#definitionpbir\"\n","        )\n","    # --- Helper: Parse the folder URL ---\n","    def parse_github_folder_url(url: str):\n","        parsed = urlparse(url)\n","        path_parts = parsed.path.strip(\"/\").split(\"/\")\n","\n","        if len(path_parts) < 2:\n","            raise ValueError(\"Invalid GitHub URL format.\")\n","\n","        owner = path_parts[0]\n","        repo = path_parts[1]\n","\n","        # Handle 'tree/main/...'\n","        if len(path_parts) > 3 and path_parts[2] in (\"tree\", \"blob\"):\n","            subpath = \"/\".join(path_parts[4:])\n","        else:\n","            subpath = \"/\".join(path_parts[2:])\n","\n","        return owner, repo, unquote(subpath)\n","\n","    # --- Helper: Recursively fetch all files and relative paths ---\n","    def get_all_files_with_relative_paths(owner: str, repo: str, base_path: str, current_path: str = None):\n","        if current_path is None:\n","            current_path = base_path\n","\n","        api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{current_path}\"\n","        response = requests.get(api_url)\n","        response.raise_for_status()\n","        items = response.json()\n","\n","        files = []\n","        for item in items:\n","            if item[\"type\"] == \"file\":\n","                relative_path = item[\"path\"].replace(base_path + \"/\", \"\")\n","                files.append({\n","                    \"raw_url\": item[\"download_url\"],\n","                    \"relative_path\": relative_path\n","                })\n","            elif item[\"type\"] == \"dir\":\n","                files.extend(get_all_files_with_relative_paths(owner, repo, base_path, item[\"path\"]))\n","        return files\n","\n","    # --- Helper: Download and encode file content to Base64 ---\n","    def download_and_encode_base64(url: str) -> str:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","        file_content = response.content  # .content was the safest option\n","        return base64.b64encode(file_content).decode(\"utf-8\")\n","\n","    # --- Main flow ---\n","    owner, repo, subpath = parse_github_folder_url(github_folder_url)\n","    files = get_all_files_with_relative_paths(owner, repo, subpath)\n","\n","    parts = []\n","    for file in files:\n","        if isReport and file['relative_path'] == 'definition.pbir':\n","            # Safely handle dicts, strings, or bytes\n","            if isinstance(report_pbir_definition, dict):\n","                report_pbir_definition_bytes = json.dumps(report_pbir_definition, indent=2).encode(\"utf-8\")\n","            elif isinstance(report_pbir_definition, str):\n","                report_pbir_definition_bytes = report_pbir_definition.encode(\"utf-8\")\n","            else:\n","                report_pbir_definition_bytes = report_pbir_definition  # assume bytes\n","            encoded = base64.b64encode(report_pbir_definition_bytes).decode(\"utf-8\")\n","        else:\n","            encoded = download_and_encode_base64(file[\"raw_url\"])\n","        parts.append({\n","            \"path\": file[\"relative_path\"],\n","            \"payload\": encoded,\n","            \"payloadType\": \"InlineBase64\"\n","        })\n","\n","    print(f\"✅ Retrieved {len(parts)} files from {repo}/{subpath}\")\n","    return parts\n","\n","##################### THIS FUNCTION DOWNLOADS DATASET TO LAKEHOUSE\n","def download_datasets(dataset_urls, save_dir=\".\"):\n","    for filename, url in dataset_urls.items():\n","        print(f\"Downloading {filename}...\")\n","        try:\n","            response = requests.get(url, timeout=15)\n","            response.raise_for_status()  # Raise exception for HTTP errors\n","            with open(f\"{save_dir}/{filename}\", \"wb\") as f:\n","                f.write(response.content)\n","            print(f\"✅ {filename} downloaded successfully.\")\n","        except requests.exceptions.RequestException as e:\n","            print(f\"❌ Failed to download {filename}: {e}\")\n","\n","################# Check if an item exists\n","def item_exists(item_name, item_type) -> bool:\n","\n","    items_df = semfabric.list_items(item_type)\n","\n","    if item_name in items_df['Display Name'].values:\n","        print(f'{item_name} of type {item_type} exists')\n","        return True\n","    else:\n","        print(f'{item_name} of type {item_type} does not exist')\n","        return False   \n","\n","################ download notebook from github and optionally attach it to a lakehouse\n","def import_notebook(notebook_import_name, githuburl, update_lakehouse=False , workspace_id=None, lakehouse_name=None ) -> str:\n","    if item_exists(notebook_import_name, \"Notebook\"):\n","        print('notebook already exists skipping download')\n","    else:\n","        # import notebook and return notebook_id\n","        result = labs.import_notebook_from_web( notebook_name=notebook_import_name, url=githuburl)\n","            \n","        # update the default lakehouse (only if allowed)\n","        if update_lakehouse:\n","            if not workspace_id or not lakehouse_name:\n","                raise ValueError(\"workspace_id and lakehouse_name must be provided when update_lakehouse=True\")\n","            notebookutils.notebook.updateDefinition(\n","                name=notebook_import_name, \n","                workspaceId=workspace_id, \n","                defaultLakehouse=lakehouse_name, \n","                defaultLakehouseWorkspace=workspace_id\n","            )\n","        \n","    # Return notebook id\n","    notebook_id = semfabric.resolve_item_id(item_name=notebook_import_name, type=\"Notebook\")\n","\n","    print(f\"notebookname: {notebook_import_name}, notebook_id: {notebook_id}\")\n","\n","    return notebook_id\n","\n","##########\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"858da73a-b8ca-4db8-81fb-e41fc187182c","normalized_state":"finished","queued_time":"2025-11-08T16:00:57.1502049Z","session_start_time":null,"execution_start_time":"2025-11-08T16:02:13.8909726Z","execution_finish_time":"2025-11-08T16:02:14.2341736Z","parent_msg_id":"3777e29f-47f5-4353-b143-be74f799cfc2"},"text/plain":"StatementMeta(, 858da73a-b8ca-4db8-81fb-e41fc187182c, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"09a4aa68-7398-4861-ae00-d4ce73750a8f"},{"cell_type":"markdown","source":["## Create Lakehouse\n","#### Created directories in the lakehouse\n","#### Mounted the file directory of the lakehouse so that we can write files to it directly\n","#### Downloaded files from Github into folders in the Lakehouse  "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2a61e4df-7241-4bae-a7e7-890997764b6b"},{"cell_type":"code","source":["# creating a new lakehouse where our data will be downladed will be downloaded into and will alsoe be used as the siver layer\n","if item_exists(lakehouse_name , 'Lakehouse'):\n","    print('lakehouse already exists, getting the lakehouse id')\n","    lakehouse_id=notebookutils.lakehouse.get(lakehouse_name, workspace_id)['id']\n","    abfs_path =f'abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}'\n","else:\n","    lakehouse = notebookutils.lakehouse.create(lakehouse_name)    \n","    lakehouse_id = lakehouse['id']\n","    abfs_path = lakehouse.get('properties',{}).get('abfsPath')\n","abfs_path"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"c8230571-7153-461c-9637-316d0f5c6677"},{"cell_type":"code","source":["# creating directories in our destination lakehouse (that this notebook is not attached to, hence the use of abfs_path) \n","notebookutils.fs.mkdirs(f'{abfs_path}/Files/{crm_data_relative_path}')\n","notebookutils.fs.mkdirs(f'{abfs_path}/Files/{erp_data_relative_path}')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"4d16b8f8-ecc5-49c8-963d-769d5f7fe324"},{"cell_type":"code","source":["##  mount the Files section of the new lakehouse \n","files_directory = abfs_path + '/Files'\n","mount_point = \"/mnt/lakehouse/\" + lakehouse_name + \"/Files\"\n","notebookutils.fs.mount(files_directory, mount_point)\n","base_dir_local_path = notebookutils.fs.getMountPath(mount_point)\n","base_dir_local_path"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"collapsed":false},"id":"f762f8df-a591-4ba5-bfca-3d39a1f979be"},{"cell_type":"code","source":["crm_data_full_local_path = f'{base_dir_local_path}/{crm_data_relative_path}'\n","erp_data_full_local_path = f'{base_dir_local_path}/{erp_data_relative_path}'\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"858da73a-b8ca-4db8-81fb-e41fc187182c","normalized_state":"finished","queued_time":"2025-11-08T16:00:58.9395898Z","session_start_time":null,"execution_start_time":"2025-11-08T16:02:29.9063625Z","execution_finish_time":"2025-11-08T16:02:30.227815Z","parent_msg_id":"5e7826b6-90b9-4e30-a011-ff2b2bbc95fd"},"text/plain":"StatementMeta(, 858da73a-b8ca-4db8-81fb-e41fc187182c, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"f63a695d-e843-46ef-ab6f-d0241efb57d9"},{"cell_type":"code","source":["download_datasets(crm_dataset_urls, crm_data_full_local_path)\n","download_datasets(erp_dataset_urls, erp_data_full_local_path)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"06cd82d6-5062-4393-a729-c21361afa183"},{"cell_type":"markdown","source":["## Create Warehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7e8bb1c6-0278-4651-b265-7adb5ed2b8ae"},{"cell_type":"code","source":["#Creating a warehouse where our gold layer views and tables will be stored\n","\n","if item_exists(warehouse_name , 'Warehouse'):\n","    print('warehouse already exists, getting the warehouse_id')\n","    df = semfabric.list_items('Warehouse')\n","    warehouse_id = df.loc[df['Display Name'] == warehouse_name, 'Id'].iloc[0]\n","else:\n","    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/warehouses\"\n","    request_body = {\"displayName\": warehouse_name}\n","    warehouse_id = fabriclient_post(url,request_body)\n","warehouse_id"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"fc9203d6-5e52-4900-90c0-a8d285451210"},{"cell_type":"markdown","source":["## Download and run transformation notebooks "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ab2c3db5-c267-42dd-a755-96e95440f091"},{"cell_type":"code","source":["#import notebooks and get Notebook Ids for the 2 notebooks to be used in subsequent steps\n","silver_layer_notebook_id = import_notebook(notebook_import_name=silver_notebook_name, githuburl=silver_notebook_url, update_lakehouse=True, workspace_id=workspace_id, lakehouse_name = lakehouse_name)\n","gold_layer_notebook_id = import_notebook(gold_notebook_name,gold_notebook_url)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"31c600bc-e6a2-4d0f-a057-24e3a63698c0"},{"cell_type":"code","source":["DAG = {\n","    \"activities\": [\n","        {\n","            \"name\": silver_notebook_name, \n","            \"path\": silver_notebook_name, \n","            \"timeoutPerCellInSeconds\": 90,\n","            \"args\": {\"useRootDefaultLakehouse\": True, \"abfs_path\":abfs_path, \"workspaceId\": workspace_id , \"lakehouseId\": lakehouse_id , \"timezone\" : timezone}\n","        },\n","        {\n","            \"name\": gold_notebook_name,\n","            \"path\": gold_notebook_name,\n","            \"timeoutPerCellInSeconds\": 90,\n","            \"args\":{\"lakehouse_name\": lakehouse_name , \"warehouse_name\":warehouse_name},\n","            \"retry\": 2, ### because it takes a while for the SQL endpoint of the lakehouse to refresh, and we will be depending on it so the first try might fail...\n","            \"retryIntervalInSeconds\": 50,\n","            \"dependencies\": [silver_notebook_name]\n","        }\n","    ],\n","    \"timeoutInSeconds\": 480, # max 4 mins for the entire pipeline\n","}\n","\n","notebookutils.notebook.runMultiple(DAG, {\"displayDAGViaGraphviz\":True})"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"73b3d107-7a1f-451b-853b-1235d781fe7b"},{"cell_type":"markdown","source":["### Create semantic model from Fabric REST API and attach it to warehouse\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21ee3b67-f933-48bf-85eb-bd5d27f77c7c"},{"cell_type":"code","source":["if item_exists(semantic_model_name, 'SemanticModel'):\n","        print('semantic model already exists, getting the ID ')\n","        df = semfabric.list_items('SemanticModel')\n","        semantic_model_id = df.loc[df['Display Name'] == semantic_model_name, 'Id'].iloc[0]\n","\n","else:\n","        semantic_model_parts = get_definition_parts_from_github(semantic_model_github_url)\n","        semantic_model_request_body= {\n","                                \"displayName\": semantic_model_name,\n","                                \"description\": \"semantic model for E2E demo\",\n","                                \"definition\":{\n","                                        \"parts\": semantic_model_parts\n","                                } \n","                            }\n","\n","        semantic_model_id = fabriclient_post(create_semantic_model_uri, semantic_model_request_body)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":"858da73a-b8ca-4db8-81fb-e41fc187182c","normalized_state":"cancelled","queued_time":"2025-11-08T16:01:00.1703645Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-08T16:03:47.6074797Z","parent_msg_id":"3fa7ac72-08c4-4a4b-a2f0-f37dfaac3a0b"},"text/plain":"StatementMeta(, 858da73a-b8ca-4db8-81fb-e41fc187182c, -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"ca2ff945-0d69-449c-8f86-43e8f3955851"},{"cell_type":"code","source":["labs.directlake.update_direct_lake_model_connection(dataset = semantic_model_name,source= warehouse_name, source_type=\"Warehouse\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":"858da73a-b8ca-4db8-81fb-e41fc187182c","normalized_state":"cancelled","queued_time":"2025-11-08T16:01:00.4397415Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-08T16:03:47.6078012Z","parent_msg_id":"1b205f02-a7c6-4c6e-b086-86afa8b4fb11"},"text/plain":"StatementMeta(, 858da73a-b8ca-4db8-81fb-e41fc187182c, -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"985e09dc-5dba-4065-ac6e-926342428a4d"},{"cell_type":"markdown","source":["## Create report "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"88b3c42a-78d4-44e7-b1bd-c4aa28c8dc3b"},{"cell_type":"code","source":["if item_exists(report_name, 'Report'):\n","        print('Report already exists, getting the ID ')\n","        df = semfabric.list_items('Report')\n","        report_id = df.loc[df['Display Name'] == report_name, 'Id'].iloc[0]\n","else:\n","    custom_pbir = {\n","        \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/item/report/definitionProperties/2.0.0/schema.json\",\n","        \"version\": \"4.0\",\n","        \"datasetReference\": {\n","            \"byConnection\": {\n","                \"connectionString\": f\"semanticmodelid={semantic_model_id}\"\n","            }\n","        }\n","    }\n","\n","    report_parts = get_definition_parts_from_github(report_github_folder, isReport=True , report_pbir_definition=custom_pbir)\n","    create_report_request_body = {\n","            \"displayName\": report_name,\n","            \"description\": \"report created using Fabric REST API\",\n","            \"definition\" : {\n","                \"parts\": report_parts\n","                }\n","            }\n","    report_id = fabriclient_post(create_report_uri, create_report_request_body)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":"858da73a-b8ca-4db8-81fb-e41fc187182c","normalized_state":"cancelled","queued_time":"2025-11-08T16:01:00.6598957Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-08T16:03:47.6080488Z","parent_msg_id":"8468fe83-1235-4188-a0d0-20d7e96540ed"},"text/plain":"StatementMeta(, 858da73a-b8ca-4db8-81fb-e41fc187182c, -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"6269c3c8-8408-4733-b077-c938ad2eb959"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"version_major":2,"version_minor":0,"state":{"a84ea54324e342ae8e79e0c9d0160f42":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"2d4427b1688a435fb0dabdfb35cc8782":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 100/100 [00:22&lt;00:00,  4.99it/s]","layout":"IPY_MODEL_2d1d37accaa143a29ebd705e71dad0bd","style":"IPY_MODEL_a84ea54324e342ae8e79e0c9d0160f42"}},"2d1d37accaa143a29ebd705e71dad0bd":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"bd18117cfff24f38bb1b145213e23dbe":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"c900e345710a4b4692d700e5b5593d9d":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"5c91d36ea92349a195b53a8f33c7c909":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"9d24e34387154d868ebd8ea77137be3f":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Operation https://api.fabric.microsoft.com/v1/workspaces/ba8ea960-6f42-4d11-a592-cb679ac58f12/warehouses successfully completed: 100%","layout":"IPY_MODEL_7f99dd5c5ff34c49ab77403d2bc27e5b","style":"IPY_MODEL_bd18117cfff24f38bb1b145213e23dbe"}},"63c0abe69075409d872c4d55c76a7751":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":100,"bar_style":"success","style":"IPY_MODEL_76f0c4b5d9b540b69beda0406b99ded9","layout":"IPY_MODEL_c900e345710a4b4692d700e5b5593d9d"}},"7f99dd5c5ff34c49ab77403d2bc27e5b":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"76f0c4b5d9b540b69beda0406b99ded9":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"2689bb0463e140daac029df60468fbc4":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_9d24e34387154d868ebd8ea77137be3f","IPY_MODEL_63c0abe69075409d872c4d55c76a7751","IPY_MODEL_2d4427b1688a435fb0dabdfb35cc8782"],"layout":"IPY_MODEL_5c91d36ea92349a195b53a8f33c7c909"}}}}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"version":"0.1","state":{}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}