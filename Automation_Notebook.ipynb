{"cells":[{"cell_type":"code","source":["%pip install semantic-link==0.11.1 semantic-link-labs==0.11.2  > /dev/null 2>&1\n","import pandas as pd\n","import requests \n","import json\n","import base64\n","import sempy_labs as labs\n","import sempy_labs.report as rep \n","import sempy.fabric as semfabric\n","from urllib.parse import urlparse, unquote\n","from typing import Union, Optional\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:33.4262283Z","session_start_time":null,"execution_start_time":"2025-11-08T09:31:34.0135631Z","execution_finish_time":"2025-11-08T09:32:22.0431412Z","parent_msg_id":"11d790a2-ae3b-4153-8faa-30384336cadd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"01ac2e86-19af-46d8-bb23-5c87b8cf0bf0"},{"cell_type":"code","source":["## You can change these as you like\n","lakehouse_name = 'Baraa_LH'\n","warehouse_name = 'Baraa_WH'\n","semantic_model_name = 'Baraa_SMM'\n","report_name = 'Baraa_Report'\n","gold_notebook_name = 'gold_layer_processing_notebook'\n","silver_notebook_name = 'silver_layer_processing_notebook'\n","timezone = \"Africa/Lagos\" # this is used for adding Data warehouse creation dates column in the transformation notebooks\n","\n","## Don't Change these ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n","crm_data_relative_path = \"datasets/source_crm\"\n","erp_data_relative_path = \"datasets/source_erp\"\n","\n","workspace_name=notebookutils.runtime.context['currentWorkspaceName']\n","workspace_id = notebookutils.runtime.context['currentWorkspaceId']\n","\n","create_semantic_model_uri = f\"v1/workspaces/{workspace_id}/semanticModels\"\n","semantic_model_github_url = \"https://github.com/maleek004/baraaE2E/tree/main/Baraa_DL_SMM.SemanticModel\"\n","semantic_model_parts = ''\n","semantic_model_request_body= ''\n","semantic_model_id = ''\n","\n","create_report_uri = f\"v1/workspaces/{workspace_id}/reports\"\n","report_github_folder = 'https://github.com/maleek004/baraaE2E/tree/main/Baraa_DL_Report.Report'\n","report_parts = ''\n","create_report_request_body = ''\n","report_id =''\n","\n","silver_notebook_url = 'https://raw.githubusercontent.com/maleek004/auto-fabric-setup-baraa/refs/heads/main/Create_silver_layer_Notebook.ipynb'\n","gold_notebook_url= 'https://raw.githubusercontent.com/maleek004/auto-fabric-setup-baraa/refs/heads/main/Load_gold_layer.ipynb'\n","\n","crm_dataset_urls = {\"cust_info.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_crm/cust_info.csv\"\n","                    ,\"prd_info.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_crm/prd_info.csv\"\n","                    ,\"sales_details.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_crm/sales_details.csv\"}\n","\n","erp_dataset_urls = {\"CUST_AZ12.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_erp/CUST_AZ12.csv\"\n","                    ,\"LOC_A101.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_erp/LOC_A101.csv\"\n","                    ,\"PX_CAT_G1V2.csv\":\"https://raw.githubusercontent.com/DataWithBaraa/sql-data-warehouse-project/refs/heads/main/datasets/source_erp/PX_CAT_G1V2.csv\"}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:33.6995732Z","session_start_time":null,"execution_start_time":"2025-11-08T09:32:22.0444717Z","execution_finish_time":"2025-11-08T09:32:22.4068751Z","parent_msg_id":"25a616d4-d04d-4e97-9749-1acd10e6d3b5"}},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"e7af4004-a6d3-4711-a00f-f385c704d1ef"},{"cell_type":"markdown","source":["## Functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64d3b43d-4e29-466b-90e5-5f3a8f763a7a"},{"cell_type":"code","source":["####################### FOR SENDING POST REQUESTS TO FABRIC REST API (WITHOUT BEARER TOKEN)\n","def fabriclient_post(url, request_body):\n","\n","    client = semfabric.FabricRestClient()\n","    \n","    response = client.request(method = \"POST\", path_or_url=url, lro_wait=True, json = request_body)\n","    print(response.status_code)\n","    print(response.text)\n","    response.raise_for_status()\n","    \n","    response_json = response.json()\n","    return response_json.get('id')\n","\n","####################### THIS FUNCTION EXTRACTS THE DEFINITION PART OF AN ITEM SYNCED TO GITHUB AND RETURNS THE FULL LIST WITH BASE64 PAYLOAD\n","\n","def get_definition_parts_from_github(github_folder_url, isReport=False, report_pbir_definition: Optional[Union[dict, str, bytes]] = None):\n","    \"\"\"\n","    Given a GitHub folder URL, recursively retrieves all files under it\n","    and returns a 'parts' list formatted like:\n","    [\n","        {\n","            \"path\": \"definition.pbism\",\n","            \"payload\": \"<base64 string>\",\n","            \"payloadType\": \"InlineBase64\"\n","        },\n","        ...\n","    ]\n","\n","    Parameters:\n","    ----------\n","    github_folder_url : str\n","        The GitHub folder URL to process.\n","    isReport : bool, optional\n","        Set to True if the folder represents a Power BI report.\n","        If True, `report_pbir_definition` must be provided.\n","    report_pbir_definition : bytes, optional\n","        The customized definition.pbir file to use (required if isReport=True).\n","\n","    Raises:\n","    -------\n","    ValueError\n","        If isReport=True but report_pbir_definition is not provided.\n","    \"\"\"\n","\n","    # ---  Validate parameters ---\n","    if isReport and report_pbir_definition is None:\n","        raise ValueError(\n","            \"Since you are trying to build a report, you must provide a custom 'definition.pbir' \"\n","            \"file that includes a 'byConnection' dataReference and the target 'semanticmodelid'.\\n\"\n","            \"For more information, visit: \"\n","            \"https://learn.microsoft.com/en-us/power-bi/developer/projects/projects-report\"\n","            \"?tabs=v2%2Cdesktop#definitionpbir\"\n","        )\n","    # --- Helper: Parse the folder URL ---\n","    def parse_github_folder_url(url: str):\n","        parsed = urlparse(url)\n","        path_parts = parsed.path.strip(\"/\").split(\"/\")\n","\n","        if len(path_parts) < 2:\n","            raise ValueError(\"Invalid GitHub URL format.\")\n","\n","        owner = path_parts[0]\n","        repo = path_parts[1]\n","\n","        # Handle 'tree/main/...'\n","        if len(path_parts) > 3 and path_parts[2] in (\"tree\", \"blob\"):\n","            subpath = \"/\".join(path_parts[4:])\n","        else:\n","            subpath = \"/\".join(path_parts[2:])\n","\n","        return owner, repo, unquote(subpath)\n","\n","    # --- Helper: Recursively fetch all files and relative paths ---\n","    def get_all_files_with_relative_paths(owner: str, repo: str, base_path: str, current_path: str = None):\n","        if current_path is None:\n","            current_path = base_path\n","\n","        api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{current_path}\"\n","        response = requests.get(api_url)\n","        response.raise_for_status()\n","        items = response.json()\n","\n","        files = []\n","        for item in items:\n","            if item[\"type\"] == \"file\":\n","                relative_path = item[\"path\"].replace(base_path + \"/\", \"\")\n","                files.append({\n","                    \"raw_url\": item[\"download_url\"],\n","                    \"relative_path\": relative_path\n","                })\n","            elif item[\"type\"] == \"dir\":\n","                files.extend(get_all_files_with_relative_paths(owner, repo, base_path, item[\"path\"]))\n","        return files\n","\n","    # --- Helper: Download and encode file content to Base64 ---\n","    def download_and_encode_base64(url: str) -> str:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","        file_content = response.content  # .content was the safest option\n","        return base64.b64encode(file_content).decode(\"utf-8\")\n","\n","    # --- Main flow ---\n","    owner, repo, subpath = parse_github_folder_url(github_folder_url)\n","    files = get_all_files_with_relative_paths(owner, repo, subpath)\n","\n","    parts = []\n","    for file in files:\n","        if isReport and file['relative_path'] == 'definition.pbir':\n","            # Safely handle dicts, strings, or bytes\n","            if isinstance(report_pbir_definition, dict):\n","                report_pbir_definition_bytes = json.dumps(report_pbir_definition, indent=2).encode(\"utf-8\")\n","            elif isinstance(report_pbir_definition, str):\n","                report_pbir_definition_bytes = report_pbir_definition.encode(\"utf-8\")\n","            else:\n","                report_pbir_definition_bytes = report_pbir_definition  # assume bytes\n","            encoded = base64.b64encode(report_pbir_definition_bytes).decode(\"utf-8\")\n","        else:\n","            encoded = download_and_encode_base64(file[\"raw_url\"])\n","        parts.append({\n","            \"path\": file[\"relative_path\"],\n","            \"payload\": encoded,\n","            \"payloadType\": \"InlineBase64\"\n","        })\n","\n","    print(f\"‚úÖ Retrieved {len(parts)} files from {repo}/{subpath}\")\n","    return parts\n","\n","##################### THIS FUNCTION DOWNLOADS DATASET TO LAKEHOUSE\n","def download_datasets(dataset_urls, save_dir=\".\"):\n","    for filename, url in dataset_urls.items():\n","        print(f\"Downloading {filename}...\")\n","        try:\n","            response = requests.get(url, timeout=15)\n","            response.raise_for_status()  # Raise exception for HTTP errors\n","            with open(f\"{save_dir}/{filename}\", \"wb\") as f:\n","                f.write(response.content)\n","            print(f\"‚úÖ {filename} downloaded successfully.\")\n","        except requests.exceptions.RequestException as e:\n","            print(f\"‚ùå Failed to download {filename}: {e}\")\n","\n","################# Check if an item exists\n","def item_exists(item_name, item_type) -> bool:\n","\n","    items_df = semfabric.list_items(item_type)\n","\n","    if item_name in items_df['Display Name'].values:\n","        print(f'{item_name} of type {item_type} exists')\n","        return True\n","    else:\n","        print(f'{item_name} of type {item_type} does not exist')\n","        return False   \n","\n","################ download notebook from github and optionally attach it to a lakehouse\n","def import_notebook(notebook_import_name, githuburl, update_lakehouse=False , workspace_id=None, lakehouse_name=None ) -> str:\n","    if item_exists(notebook_import_name, \"Notebook\"):\n","        print('notebook already exists skipping download')\n","    else:\n","        # import notebook and return notebook_id\n","        result = labs.import_notebook_from_web( notebook_name=notebook_import_name, url=githuburl)\n","            \n","        # update the default lakehouse (only if allowed)\n","        if update_lakehouse:\n","            if not workspace_id or not lakehouse_name:\n","                raise ValueError(\"workspace_id and lakehouse_name must be provided when update_lakehouse=True\")\n","            notebookutils.notebook.updateDefinition(\n","                name=notebook_import_name, \n","                workspaceId=workspace_id, \n","                defaultLakehouse=lakehouse_name, \n","                defaultLakehouseWorkspace=workspace_id\n","            )\n","        \n","    # Return notebook id\n","    notebook_id = semfabric.resolve_item_id(item_name=notebook_import_name, type=\"Notebook\")\n","\n","    print(f\"notebookname: {notebook_import_name}, notebook_id: {notebook_id}\")\n","\n","    return notebook_id\n","\n","##########\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:33.9358967Z","session_start_time":null,"execution_start_time":"2025-11-08T09:32:22.4083927Z","execution_finish_time":"2025-11-08T09:32:22.7639913Z","parent_msg_id":"4d7b18f4-2d76-401c-ac97-0200ac930a87"}},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"f2422e7c-5748-400c-b9e8-58ceff385819"},{"cell_type":"markdown","source":["## Create Lakehouse\n","#### Created directories in the lakehouse\n","#### Mounted the file directory of the lakehouse so that we can write files to it directly\n","#### Downloaded files from Github into folders in the Lakehouse  "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cab20839-2b9a-472d-a6b3-47f2017835cc"},{"cell_type":"code","source":["# creating a new lakehouse where our data will be downladed will be downloaded into and will alsoe be used as the siver layer\n","if item_exists(lakehouse_name , 'Lakehouse'):\n","    print('lakehouse already exists, getting the lakehouse id')\n","    lakehouse_id=notebookutils.lakehouse.get(lakehouse_name, workspace_id)['id']\n","    abfs_path =f'abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}'\n","else:\n","    lakehouse = notebookutils.lakehouse.create(lakehouse_name)    \n","    lakehouse_id = lakehouse['id']\n","    abfs_path = lakehouse.get('properties',{}).get('abfsPath')\n","abfs_path"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:34.3986787Z","session_start_time":null,"execution_start_time":"2025-11-08T09:32:22.7650883Z","execution_finish_time":"2025-11-08T09:32:28.4843462Z","parent_msg_id":"1e1f78c8-8f0e-45a2-9582-474f9be082bf"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Baraa_LH of type Lakehouse exists\nlakehouse already exists, getting the lakehouse id\n"]},{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"'abfss://a8090863-3315-4b19-9138-b7069c51339a@onelake.dfs.fabric.microsoft.com/ce8d3b4b-b9b1-4fc3-9434-a6c833364cdf'"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"5e3ab6bc-b89e-4d65-bc92-f310638d70ae"},{"cell_type":"code","source":["# creating directories in our destination lakehouse (that this notebook is not attached to, hence the use of abfs_path) \n","notebookutils.fs.mkdirs(f'{abfs_path}/Files/{crm_data_relative_path}')\n","notebookutils.fs.mkdirs(f'{abfs_path}/Files/{erp_data_relative_path}')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:34.6831023Z","session_start_time":null,"execution_start_time":"2025-11-08T09:32:28.4856113Z","execution_finish_time":"2025-11-08T09:32:30.614781Z","parent_msg_id":"b5c6a8cd-bde7-463b-a056-0637de6e099c"}},"metadata":{}},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"78bac567-be85-457b-81de-133f43ca3ca6"},{"cell_type":"code","source":["##  mount the Files section of the new lakehouse \n","files_directory = abfs_path + '/Files'\n","mount_point = \"/mnt/lakehouse/\" + lakehouse_name + \"/Files\"\n","notebookutils.fs.mount(files_directory, mount_point)\n","base_dir_local_path = notebookutils.fs.getMountPath(mount_point)\n","base_dir_local_path"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:34.8954735Z","session_start_time":null,"execution_start_time":"2025-11-08T09:32:30.6160167Z","execution_finish_time":"2025-11-08T09:32:34.8135942Z","parent_msg_id":"54f150cc-1826-4231-8138-27013dc54e12"}},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse-jupyter.display-view+json":{"table":{"rows":[{"0":"abfss://a8090863-3315-4b19-9138-b7069c51339a@onelake.dfs.fabric.microsoft.com/ce8d3b4b-b9b1-4fc3-9434-a6c833364cdf/Files","1":"/mnt/lakehouse/Baraa_LH/Files","2":"job","3":"Lakehouse","4":"/synfs/notebook/0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911/mnt/lakehouse/Baraa_LH/Files"}],"schema":[{"key":"0","name":"source","type":"string"},{"key":"1","name":"mountPoint","type":"string"},{"key":"2","name":"scope","type":"string"},{"key":"3","name":"storageType","type":"string"},{"key":"4","name":"localPath","type":"string"}],"truncated":false},"isSummary":false,"language":"python"}},"metadata":{}},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"'/synfs/notebook/0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911/mnt/lakehouse/Baraa_LH/Files'"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"e554749f-4533-49e8-9c03-d5093990570c"},{"cell_type":"code","source":["crm_data_full_local_path = f'{base_dir_local_path}/{crm_data_relative_path}'\n","erp_data_full_local_path = f'{base_dir_local_path}/{erp_data_relative_path}'\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:35.0913886Z","session_start_time":null,"execution_start_time":"2025-11-08T09:32:34.8147973Z","execution_finish_time":"2025-11-08T09:32:35.2115257Z","parent_msg_id":"ec98a34d-cd81-4a53-8ae0-a55c1507b99e"}},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"b781f4c2-bf2b-46b5-94f3-6429389c69e5"},{"cell_type":"code","source":["download_datasets(crm_dataset_urls, crm_data_full_local_path)\n","download_datasets(erp_dataset_urls, erp_data_full_local_path)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:35.2529703Z","session_start_time":null,"execution_start_time":"2025-11-08T09:32:35.2127791Z","execution_finish_time":"2025-11-08T09:32:38.4796743Z","parent_msg_id":"4500d032-ba5a-429c-a2d2-0137017bafb3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading cust_info.csv...\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ cust_info.csv downloaded successfully.\nDownloading prd_info.csv...\n‚úÖ prd_info.csv downloaded successfully.\nDownloading sales_details.csv...\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ sales_details.csv downloaded successfully.\nDownloading CUST_AZ12.csv...\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ CUST_AZ12.csv downloaded successfully.\nDownloading LOC_A101.csv...\n‚úÖ LOC_A101.csv downloaded successfully.\nDownloading PX_CAT_G1V2.csv...\n‚úÖ PX_CAT_G1V2.csv downloaded successfully.\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"50121c39-73c5-4b4c-ae6e-1b881dcb3cf4"},{"cell_type":"markdown","source":["## Create Warehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c11633c8-621d-4140-8599-aa50ef5650ec"},{"cell_type":"code","source":["#Creating a warehouse where our gold layer views and tables will be stored\n","\n","if item_exists(warehouse_name , 'Warehouse'):\n","    print('warehouse already exists, getting the warehouse_id')\n","    df = semfabric.list_items('Warehouse')\n","    warehouse_id = df.loc[df['Display Name'] == warehouse_name, 'Id'].iloc[0]\n","else:\n","    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/warehouses\"\n","    request_body = {\"displayName\": warehouse_name}\n","    warehouse_id = fabriclient_post(url,request_body)\n","warehouse_id"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:35.4865807Z","session_start_time":null,"execution_start_time":"2025-11-08T09:32:38.480896Z","execution_finish_time":"2025-11-08T09:32:38.8482658Z","parent_msg_id":"edc02e55-bb0b-4147-9352-428c33899d79"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Baraa_WH of type Warehouse exists\nwarehouse already exists, getting the warehouse_id\n"]},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"'4bd1c881-36fd-45b0-b8a3-baec8741a9ef'"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"7ca1f3a5-3afb-404d-9b59-d63b48600059"},{"cell_type":"markdown","source":["## Download and run transformation notebooks "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"79154a14-274f-4324-a12a-392cd7ad9015"},{"cell_type":"code","source":["#import notebooks and get Notebook Ids for the 2 notebooks to be used in subsequent steps\n","silver_layer_notebook_id = import_notebook(notebook_import_name=silver_notebook_name, githuburl=silver_notebook_url, update_lakehouse=True, workspace_id=workspace_id, lakehouse_name = lakehouse_name)\n","gold_layer_notebook_id = import_notebook(gold_notebook_name,gold_notebook_url)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:35.6225908Z","session_start_time":null,"execution_start_time":"2025-11-08T09:32:38.8494779Z","execution_finish_time":"2025-11-08T09:32:39.652595Z","parent_msg_id":"b12c4575-932f-45ce-8fcd-13b8fdcaaad9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["silver_layer_processing_notebook of type Notebook exists\nnotebook already exists skipping download\nnotebookname: silver_layer_processing_notebook, notebook_id: 48843d25-b416-48ac-97e5-23d45d4cc631\ngold_layer_processing_notebook of type Notebook exists\nnotebook already exists skipping download\nnotebookname: gold_layer_processing_notebook, notebook_id: 4ee5720c-83f0-4145-bcec-f2bdecb6a67e\n"]}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"ef344463-e33e-4b72-8d42-9fa4c09ef0ba"},{"cell_type":"code","source":["DAG = {\n","    \"activities\": [\n","        {\n","            \"name\": silver_notebook_name, \n","            \"path\": silver_notebook_name, \n","            \"timeoutPerCellInSeconds\": 90,\n","            \"args\": {\"useRootDefaultLakehouse\": True, \"abfs_path\":abfs_path, \"workspaceId\": workspace_id , \"lakehouseId\": lakehouse_id , \"timezone\" : timezone}\n","        },\n","        {\n","            \"name\": gold_notebook_name,\n","            \"path\": gold_notebook_name,\n","            \"timeoutPerCellInSeconds\": 90,\n","            \"args\":{\"lakehouse_name\": lakehouse_name , \"warehouse_name\":warehouse_name},\n","            \"retry\": 2, ### because it takes a while for the SQL endpoint of the lakehouse to refresh, and we will be depending on it so the first try might fail...\n","            \"retryIntervalInSeconds\": 50,\n","            \"dependencies\": [silver_notebook_name]\n","        }\n","    ],\n","    \"timeoutInSeconds\": 480, # max 4 mins for the entire pipeline\n","}\n","\n","notebookutils.notebook.runMultiple(DAG, {\"displayDAGViaGraphviz\":True})"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:35.7924421Z","session_start_time":null,"execution_start_time":"2025-11-08T09:32:39.6538679Z","execution_finish_time":"2025-11-08T09:33:41.5176781Z","parent_msg_id":"b46190e5-613c-438a-9851-c5a35c64a3b6"}},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.mssparkutilsrunmultiple-result+json":{"activities":[{"run_id":"36959b32-2dca-4d5a-91c5-7cb763d1b638","in_pipeline":false,"notebook_name":"silver_layer_processing_notebook","session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","spark_pool":"","capacity_id":"A9771750-F3DC-46B4-BF1F-06EBC4AFCEED","workspace_id":"a8090863-3315-4b19-9138-b7069c51339a","root_artifact_id":"819dd808-3b38-4848-a8b1-00cd3f392b37","artifact_id":"48843d25-b416-48ac-97e5-23d45d4cc631","activity_name":"silver_layer_processing_notebook","status":"success","status_msg":"Success","progress":100,"exit_value":"DONE üëçüèΩüëçüèΩ","exception":"","args":{"abfs_path":{"type":"string","value":"abfss://a8090863-3315-4b19-9138-b7069c51339a@onelake.dfs.fabric.microsoft.com/ce8d3b4b-b9b1-4fc3-9434-a6c833364cdf"},"workspaceId":{"type":"string","value":"a8090863-3315-4b19-9138-b7069c51339a"},"lakehouseId":{"type":"string","value":"ce8d3b4b-b9b1-4fc3-9434-a6c833364cdf"},"timezone":{"type":"string","value":"Africa/Lagos"}},"start_time":1762594361085.839,"end_time":1762594389588.0017,"duration":28502.16269493103,"snapshot_status":"success","snapshot_error":"","created_time":1762594361085.839},{"run_id":"e24ff9dd-6d84-4608-adc8-dcba88b9901f","in_pipeline":false,"notebook_name":"gold_layer_processing_notebook","session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","spark_pool":"","capacity_id":"A9771750-F3DC-46B4-BF1F-06EBC4AFCEED","workspace_id":"a8090863-3315-4b19-9138-b7069c51339a","root_artifact_id":"819dd808-3b38-4848-a8b1-00cd3f392b37","artifact_id":"4ee5720c-83f0-4145-bcec-f2bdecb6a67e","activity_name":"gold_layer_processing_notebook","status":"success","status_msg":"Success","progress":100,"exit_value":"DONE üëçüèΩüëçüèΩ","exception":"","args":{"lakehouse_name":{"type":"string","value":"Baraa_LH"},"warehouse_name":{"type":"string","value":"Baraa_WH"}},"start_time":1762594389588.4104,"end_time":1762594419490.3403,"duration":29901.92985534668,"snapshot_status":"success","snapshot_error":"","created_time":1762594389588.4104}],"numbers":{"pending":0,"running":0,"failed":0,"succeeded":2},"limit":50}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"228pt\" height=\"116pt\" viewBox=\"0.00 0.00 228.00 116.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 224,-112 224,4 -4,4\"/>\n<!-- silver_layer_processing_notebook -->\n<g id=\"node1\" class=\"node\">\n<title>silver_layer_processing_notebook</title>\n<g id=\"a_node1\"><a xlink:title=\"Name: silver_layer_processing_notebook\nArgs: abfs_path={'type': 'string', 'value': 'abfss://a8090863-3315-4b19-9138-b7069c51339a@onelake.dfs.fabric.microsoft.com/ce8d3b4b-b9b1-4fc3-9434-a6c833364cdf'}\n ¬†workspaceId={'type': 'string', 'value': 'a8090863-3315-4b19-9138-b7069c51339a'}\n ¬†lakehouseId={'type': 'string', 'value': 'ce8d3b4b-b9b1-4fc3-9434-a6c833364cdf'}\n ¬†timezone={'type': 'string', 'value': 'Africa/Lagos'}\nTime: 2025-11-08 09:32:41 -&gt; 2025-11-08 09:33:09, Duration: 28.50216269493103s\nStatus: Success (100%)\">\n<polygon fill=\"none\" stroke=\"#107c10\" points=\"220,-108 0,-108 0,-72 220,-72 220,-108\"/>\n<text text-anchor=\"middle\" x=\"110\" y=\"-84.58\" font-family=\"Times,serif\" font-size=\"14.00\">silver_layer_processing_notebook</text>\n</a>\n</g>\n</g>\n<!-- gold_layer_processing_notebook -->\n<g id=\"node2\" class=\"node\">\n<title>gold_layer_processing_notebook</title>\n<g id=\"a_node2\"><a xlink:title=\"Name: gold_layer_processing_notebook\nArgs: lakehouse_name={'type': 'string', 'value': 'Baraa_LH'}\n ¬†warehouse_name={'type': 'string', 'value': 'Baraa_WH'}\nTime: 2025-11-08 09:33:09 -&gt; 2025-11-08 09:33:39, Duration: 29.90192985534668s\nStatus: Success (100%)\">\n<polygon fill=\"none\" stroke=\"#107c10\" points=\"217,-36 3,-36 3,0 217,0 217,-36\"/>\n<text text-anchor=\"middle\" x=\"110\" y=\"-12.57\" font-family=\"Times,serif\" font-size=\"14.00\">gold_layer_processing_notebook</text>\n</a>\n</g>\n</g>\n<!-- silver_layer_processing_notebook&#45;&gt;gold_layer_processing_notebook -->\n<g id=\"edge1\" class=\"edge\">\n<title>silver_layer_processing_notebook-&gt;gold_layer_processing_notebook</title>\n<path fill=\"none\" stroke=\"black\" d=\"M110,-71.7C110,-64.24 110,-55.32 110,-46.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"113.5,-47.1 110,-37.1 106.5,-47.1 113.5,-47.1\"/>\n</g>\n</g>\n</svg>"},"metadata":{}},{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"{'silver_layer_processing_notebook': {'exitVal': 'DONE üëçüèΩüëçüèΩ',\n  'exception': None,\n  'runId': '36959b32-2dca-4d5a-91c5-7cb763d1b638'},\n 'gold_layer_processing_notebook': {'exitVal': 'DONE üëçüèΩüëçüèΩ',\n  'exception': None,\n  'runId': 'e24ff9dd-6d84-4608-adc8-dcba88b9901f'}}"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"3b93bde9-750c-4871-8e65-5e49ec02478b"},{"cell_type":"markdown","source":["### Create semantic model from Fabric REST API and attach it to warehouse\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"63efa15d-41d6-4053-86ea-dbe03acd1644"},{"cell_type":"code","source":["if item_exists(semantic_model_name, 'SemanticModel'):\n","        print('semantic model already exists, getting the ID ')\n","        df = semfabric.list_items('SemanticModel')\n","        semantic_model_id = df.loc[df['Display Name'] == semantic_model_name, 'Id'].iloc[0]\n","\n","else:\n","        semantic_model_parts = get_definition_parts_from_github(semantic_model_github_url)\n","        semantic_model_request_body= {\n","                                \"displayName\": semantic_model_name,\n","                                \"description\": \"semantic model for E2E demo\",\n","                                \"definition\":{\n","                                        \"parts\": semantic_model_parts\n","                                } \n","                            }\n","\n","        semantic_model_id = fabriclient_post(create_semantic_model_uri, semantic_model_request_body)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:35.9174002Z","session_start_time":null,"execution_start_time":"2025-11-08T09:33:41.5188407Z","execution_finish_time":"2025-11-08T09:33:41.8877742Z","parent_msg_id":"4c854605-4992-4413-81c4-a17ce7d83241"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Baraa_SMM of type SemanticModel exists\nsemantic model already exists, getting the ID \n"]}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"e754d529-a2e0-4c31-a824-abfcfc0d5292"},{"cell_type":"code","source":["labs.directlake.update_direct_lake_model_connection(dataset = semantic_model_name,source= warehouse_name, source_type=\"Warehouse\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:36.1015126Z","session_start_time":null,"execution_start_time":"2025-11-08T09:33:41.8890337Z","execution_finish_time":"2025-11-08T09:33:59.0660796Z","parent_msg_id":"7225fca4-bcf1-4704-9221-8d72e7c0b558"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üü¢ The expression in the 'Baraa_SMM' semantic model within the 'test10k' workspace has been updated to point to the 'Baraa_WH' warehouse in the 'test10k' workspace.\n"]}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"a01f077c-42ca-4bc7-8757-9318fb0b9105"},{"cell_type":"markdown","source":["## Create report "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0492962d-0d58-402b-8ba6-104f0fd4fead"},{"cell_type":"code","source":["if item_exists(report_name, 'Report'):\n","        print('Report already exists, getting the ID ')\n","        df = semfabric.list_items('Report')\n","        report_id = df.loc[df['Display Name'] == report_name, 'Id'].iloc[0]\n","else:\n","    custom_pbir = {\n","        \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/item/report/definitionProperties/2.0.0/schema.json\",\n","        \"version\": \"4.0\",\n","        \"datasetReference\": {\n","            \"byConnection\": {\n","                \"connectionString\": f\"semanticmodelid={semantic_model_id}\"\n","            }\n","        }\n","    }\n","\n","    report_parts = get_definition_parts_from_github(report_github_folder, isReport=True , report_pbir_definition=custom_pbir)\n","    create_report_request_body = {\n","            \"displayName\": report_name,\n","            \"description\": \"report created using Fabric REST API\",\n","            \"definition\" : {\n","                \"parts\": report_parts\n","                }\n","            }\n","    report_id = fabriclient_post(create_report_uri, create_report_request_body)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"0ac26bdf-ec5d-46f2-a7f1-cdffcdbac911","normalized_state":"finished","queued_time":"2025-11-08T09:31:36.202514Z","session_start_time":null,"execution_start_time":"2025-11-08T09:33:59.0673858Z","execution_finish_time":"2025-11-08T09:33:59.4252469Z","parent_msg_id":"fee309e0-0c91-4243-9aab-f71f67980b7b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Baraa_Report of type Report exists\nReport already exists, getting the ID \n"]}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"ca5e3f27-c067-49e7-ba88-c2c4317975ad"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}